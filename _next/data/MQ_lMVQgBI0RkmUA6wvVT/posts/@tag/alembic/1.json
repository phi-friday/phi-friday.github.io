{"pageProps":{"tag":"alembic","page":1,"max_page":1,"tag_counter":[["@all",25],["python",21],["fastapi",14],["sqlmodel",8],["fastapi-users",4],["crud",4],["returns",3],["함수형 프로그래밍",3],["windows",3],["wsl",3],["tdd",2],["anyio",2],["async",2],["vim",1],["js",1],["ts",1],["nextjs",1],["velog",1],["github",1],["restful",1],["pytest",1],["alembic",1],["postgres",1],["black",1],["isort",1],["vscode",1],["asyncio",1],["trio",1]],"posts":[{"name":"fastapi 튜토리얼 -2- sql 서버 연결","content":"\n**`fastapi`** 사용법을 다시 공부할겸, 참고할만한 좋은 [예제](https://www.jeffastor.com/blog/populating-cleaning-jobs-with-user-offers-in-fastapi)가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다.\n지난번처럼 어쩌다 그만둘 수도 있긴 하지만...\n\n---\n\n## `sql` 서버 연결을 위한 초기 설정\n\n### `docker-compose` 설정\n\nsql 서버는 **`postgres`** 를 사용하기로 한다. 따라서 필요한 파이썬 패키지를 추가로 설치하고, `docker-compose.yml` 파일을 수정한다.\n\n```bash\n❯ poetry add asyncpg sqlalchemy sqlmodel\n❯ poetry export -f requirements.txt --output backend/requirements.txt --without-hashes\n```\n\n```yaml\n# docker-compose.yml\n# prettier-ignore\nversion: \"3.8\"\nservices:\n  server:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile\n    volumes:\n      - ./backend/:/backend/\n    command: uvicorn app.api.server:app --reload --workers 2 --host 0.0.0.0 --port 8000\n    env_file:\n      - ./backend/.env\n    ports:\n      - 8000:8000\n    depends_on:\n      - db\n\n  db:\n    image: postgres:14-alpine\n    volumes:\n      - ./postgres_data:/var/lib/postgresql/data/\n    env_file:\n      - ./backend/.env\n    ports:\n      - 5432:5432\n```\n\n### `git` 구성\n\n이유는 모르겠지만, **jeffastor**는 이전 글부터가 아닌, 이번 글 부터 **`git`** 으로 관리를 시작한다..\n\n```bash\n❯ touch .gitignore\n```\n\n```yaml\n# .gitignore\n# Byte-compiled files\n__pycache__/\n# Environment files\n.env\n```\n\n```bash\n❯ git init\n❯ git add .\n❯ git commit -m \"Dockerized FastAPI app with postgres.\"\n```\n\n### 환경변수 설정\n\n**`postgres`** 및 서버 전반적으로 사용할 환경변수를 설정한다.\n\n```python\n# backend/.env\nSECRET_KEY=supersecret\n\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=postgres\nPOSTGRES_SERVER=db\nPOSTGRES_PORT=5432\nPOSTGRES_DB=postgres\n```\n\n`SECRET_KEY`의 경우, 지금은 기본값으로 사용하지만 나중에 수정할거니 걱정하지 않아도 된다고 한다..\n\n### `config.py` 설정\n\n이제 서버에서 사용할 설정 파일을 생성한다.\n\n```bash\n❯ touch backend/app/core/config.py\n```\n\n```python\n# backend/app/core/config.py\nfrom sqlalchemy.engine.url import URL\nfrom starlette.config import Config\nfrom starlette.datastructures import Secret\n\nconfig = Config(\".env\")\n\nPROJECT_NAME = \"jeffastor_tutor\"\nVERSION = \"1.0.0\"\nAPI_PREFIX = \"/api\"\n\nSECRET_KEY = config(\"SECRET_KEY\", cast=Secret, default=\"CHANGEME\")\n\nPOSTGRES_USER = config(\"POSTGRES_USER\", cast=str)\nPOSTGRES_PASSWORD = config(\"POSTGRES_PASSWORD\", cast=Secret)\nPOSTGRES_SERVER = config(\"POSTGRES_SERVER\", cast=str, default=\"db\")\nPOSTGRES_PORT = config(\"POSTGRES_PORT\", cast=int, default=5432)\nPOSTGRES_DB = config(\"POSTGRES_DB\", cast=str)\n\nDATABASE_URL = config(\n    \"DATABASE_URL\",\n    cast=str,\n    default=URL.create(\n        drivername=\"postgresql+asyncpg\",\n        username=POSTGRES_USER,\n        password=POSTGRES_PASSWORD,\n        host=POSTGRES_SERVER,\n        port=POSTGRES_PORT,\n        database=POSTGRES_DB,\n    ).render_as_string(hide_password=False),\n)\n```\n\n원 예제와 다르게, **`sqlmodel`** 을 사용할 예정이라 url을 다르게 설정했다.\n\n> 설명에 따르면, 기본값이 없는 `config` 객체에 설정된 모든 값은 `.env`파일에서 값을 제공해야하고, 그렇지 않으면 에러가 발생한다고 한다.\n\n## `sql` 서버 연결 스크립트 작성\n\n이제 sql 서버와 연결하기 위한 모듈과 앱 시작/종료 이벤트와 관련한 작업 파일을 생성한다.\n\n```bash\n❯ mkdir backend/app/db\n❯ touch backend/app/db/__init__.py backend/app/db/tasks.py backend/app/db/engine.py  backend/app/core/tasks.py\n```\n\n### 엔진 설정\n\n```python\n# backend/app/db/engine.py\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.pool import QueuePool\n\nfrom ..core.config import DATABASE_URL\n\nengine = create_async_engine(\n    DATABASE_URL, pool_size=10, poolclass=QueuePool, pool_pre_ping=True\n)\n```\n\n```python\n# backend/app/db/tasks.py\nimport logging\nfrom typing import cast\n\nfrom fastapi import FastAPI\nfrom sqlalchemy.ext.asyncio.engine import AsyncEngine\n\nfrom .engine import engine\n\nlogger = logging.getLogger(__name__)\n\n\nasync def connect_to_db(app: FastAPI) -> None:\n    try:\n        async with engine.connect():\n            logger.info(\n                f\"connected db: {engine.url.render_as_string(hide_password=True)}\"\n            )\n        app.state._db = engine\n    except Exception as e:\n        logger.warning(\"--- DB CONNECTION ERROR ---\")\n        logger.warning(e)\n        logger.warning(\"--- DB CONNECTION ERROR ---\")\n\n\nasync def close_db_connection(app: FastAPI) -> None:\n    engine = cast(AsyncEngine, app.state._db)\n    try:\n        await engine.dispose()\n    except Exception as e:\n        logger.warning(\"--- DB DISCONNECT ERROR ---\")\n        logger.warning(e)\n        logger.warning(\"--- DB DISCONNECT ERROR ---\")\n```\n\n```python\n# backend/app/core/tasks.py\nfrom typing import Any, Callable, Coroutine\n\nfrom fastapi import FastAPI\n\nfrom ..db.tasks import close_db_connection, connect_to_db\n\n\ndef create_start_app_handler(app: FastAPI) -> Callable[[], Coroutine[Any, Any, None]]:\n    async def start_app() -> None:\n        await connect_to_db(app)\n\n    return start_app\n\n\ndef create_stop_app_handler(app: FastAPI) -> Callable[[], Coroutine[Any, Any, None]]:\n    async def stop_app() -> None:\n        await close_db_connection(app)\n\n    return stop_app\n```\n\n`AsyncEngine` 인스턴스를 생성하고, 이 인스턴스를 이용해서 앱을 시작할 때와 종료할 때 실행할 두가지 핸들러를 정의했다.\n\n> 기존 글에서는 엔진을 직접 만들기 보다 **`databases`** 를 이용하는데, 일단 이전에 사용한 적 있는 **`sqlmodel`** 로 진행한다.\n\n이 핸들러는 sql 서버 연결이 정상적으로 이루어졌다면, 앱의 `state`에 `_db`라는 속성으로 `AsyncEngine` 인스턴스를 호출할 수 있게 한다.\n그리고 종료할 때, 이 인스턴스와 연결된 모든 세션을 종료한다.\n이제 이 핸들러를 적용한다.\n\n```python\n# backend/app/api/routes/server.py\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import ORJSONResponse\n\nfrom ..core import config, tasks\nfrom .routes import router as api_router\n\n\ndef get_application() -> FastAPI:\n    app = FastAPI(\n        title=config.PROJECT_NAME,\n        version=config.VERSION,\n        default_response_class=ORJSONResponse,\n    )\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    app.add_event_handler(\"startup\", tasks.create_start_app_handler(app))\n    app.add_event_handler(\"shutdown\", tasks.create_stop_app_handler(app))\n\n    app.include_router(api_router, prefix=config.API_PREFIX)\n\n    return app\n\n\napp = get_application()\n```\n\n만약 **`docker`** 실행 시 정상적으로 작동한다면, sql 서버를 사용할 준비가 끝났다.\n\n### `alembic`을 사용한 마이그레이션\n\n이제 **`alembic`** 을 사용한 마이그레이션을 구성한다고 하는데.. 사용해본적이 한번도 없어서 생소한 라이브러리다. 그러니 일단 그대로 따라하는데 중점을 둔다.\n\n```bash\n❯ mkdir backend/app/db/migrations backend/app/db/repositories\n❯ touch backend/app/db/migrations/script.py.mako backend/app/db/migrations/env.py backend/app/db/repositories/__init__.py backend/app/db/repositories/base.py backend/alembic.ini\n```\n\n> **`mako`** 확장자는 [Mako](https://www.makotemplates.org/) 템플릿이라고 한다.\n\n```yaml\n# backend/alembic.ini\n# A generic, single database configuration.\n\n[alembic]\n# path to migration scripts\nscript_location = ./app/db/migrations\n\n# template used to generate migration files\n# file_template = %%(rev)s_%%(slug)s\n\n# sys.path path, will be prepended to sys.path if present.\n# defaults to the current working directory.\n# prepend_sys_path = .\n\n# timezone to use when rendering the date within the migration file\n# as well as the filename.\n# If specified, requires the python-dateutil library that can be\n# installed by adding `alembic[tz]` to the pip requirements\n# string value is passed to dateutil.tz.gettz()\n# leave blank for localtime\n# timezone =\n\n# max length of characters to apply to the\n# \"slug\" field\n# truncate_slug_length = 40\n\n# set to 'true' to run the environment during\n# the 'revision' command, regardless of autogenerate\n# revision_environment = false\n\n# set to 'true' to allow .pyc and .pyo files without\n# a source .py file to be detected as revisions in the\n# versions/ directory\n# sourceless = false\n\n# version location specification; This defaults\n# to test/versions.  When using multiple version\n# directories, initial revisions must be specified with --version-path.\n# The path separator used here should be the separator specified by \"version_path_separator\" below.\n# version_locations = %(here)s/bar:%(here)s/bat:test/versions\nversion_locations = ./app/db/migrations/versions\n\n# version path separator; As mentioned above, this is the character used to split\n# version_locations. The default within new alembic.ini files is \"os\", which uses os.pathsep.\n# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.\n# Valid values for version_path_separator are:\n#\n# version_path_separator = :\n# version_path_separator = ;\n# version_path_separator = space\n# version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.\n\n# the output encoding used when revision files\n# are written from script.py.mako\n# output_encoding = utf-8\n\n# sqlalchemy.url = driver://user:pass@localhost/dbname\n\n\n[post_write_hooks]\n# post_write_hooks defines scripts or Python functions that are run\n# on newly generated revision scripts.  See the documentation for further\n# detail and examples\n\n# format using \"black\" - use the bash_scripts runner, against the \"black\" entrypoint\n# hooks = black\n# black.type = bash_scripts\n# black.entrypoint = black\n# black.options = -l 79 REVISION_SCRIPT_FILENAME\n\n# Logging configuration\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = bash\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = bash\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_bash]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n```\n\n```python\n# backend/app/db/migrations/script.py.mako\n\"\"\"${message}\n\nRevision ID: ${up_revision}\nRevises: ${down_revision | comma,n}\nCreate Date: ${create_date}\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n${imports if imports else \"\"}\n\n# revision identifiers, used by Alembic.\nrevision = ${repr(up_revision)}\ndown_revision = ${repr(down_revision)}\nbranch_labels = ${repr(branch_labels)}\ndepends_on = ${repr(depends_on)}\n\n\ndef upgrade():\n    ${upgrades if upgrades else \"pass\"}\n\n\ndef downgrade():\n    ${downgrades if downgrades else \"pass\"}\n```\n\n각종 설정이 추가되는데, **`mako`** 템플릿 파일은 마이그레이션 스크립트를 생성하고, 그 과정을 로그로 남기는 것이라고 한다... 실제로 봐야 알 수 있을 듯.\n\n이제 끝으로 `env.py`를 작성해야하는데, 이제보니 첫 과정에서 **`alembic`** 를 추가하지 않았기에, 그 과정을 함께한다.\n\n```bash\n❯ poetry add alembic\n❯ poetry export -f requirements.txt --output backend/requirements.txt --without-hashes\n```\n\n```python\n# backend/app/db/migrations/env.py\nimport asyncio\nimport logging\nimport pathlib\nimport sys\nfrom logging.config import fileConfig\nfrom typing import cast\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlalchemy.future.engine import Engine\n\nsys.path.append(str(pathlib.Path(__file__).resolve().parents[3]))\nfrom app.core.config import DATABASE_URL  # noqa\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\nlogger = logging.getLogger(\"alembic.env\")\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = None\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline():\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    # url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        # url=url,\n        url=DATABASE_URL,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection):\n    context.configure(connection=connection, target_metadata=target_metadata)\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\nasync def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    config.set_main_option(\"sqlalchemy.url\", DATABASE_URL)\n    connectable = AsyncEngine(\n        cast(\n            Engine,\n            engine_from_config(\n                config.get_section(config.config_ini_section),\n                prefix=\"sqlalchemy.\",\n                poolclass=pool.NullPool,\n                future=True,\n            ),\n        )\n    )\n\n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n\n    await connectable.dispose()\n\n\nif context.is_offline_mode():\n    logger.info(\"Running migrations offline\")\n    run_migrations_offline()\nelse:\n    logger.info(\"Running migrations online\")\n    asyncio.run(run_migrations_online())\n```\n\n이제 첫번째 마이그레이션을 **`docker`** 내부에서 실행하면 된다고 하는데...\n\n```bash\nroot@bad23fe368a6:/backend# ls\nDockerfile  alembic.ini  app  requirements.txt  tests\nroot@bad23fe368a6:/backend# alembic revision -m \"create account table\"\n  Generating\n  /backend/app/db/migrations/versions/f721febf752b_create_account_table.py\n  ...  done\n```\n\n마이그레이션이 정상적으로 진행됐다!\n그리고 다음과 같은 파일을 확인할 수 있다.\n\n```python\n# backend/app/db/migrations/versions/f721febf752b_create_account_table.py\n\"\"\"create account table\n\nRevision ID: f721febf752b\nRevises:\nCreate Date: 2022-04-27 17:21:25.945460\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'f721febf752b'\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    pass\n\n\ndef downgrade():\n    pass\n```\n\n### 마이그레이션 테스트 모델 생성 및 확인\n\n`env.py` 파일의\n\n```python\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = None\n```\n\n주석에 적혀있듯이, `autogenerate` 옵션을 사용하면 자동으로 모델 기반으로 생성해준다고 한다. 다만 이 기능이 완벽하지는 않은지, 원 작성자 **jeffastor**는 마이그레이션으로 작성된 스크립트에\n\n```python\ndef create_cleanings_table() -> None:\n    op.create_table(\n        \"cleanings\",\n        sa.Column(\"id\", sa.Integer, primary_key=True),\n        sa.Column(\"name\", sa.Text, nullable=False, index=True),\n        sa.Column(\"description\", sa.Text, nullable=True),\n        sa.Column(\"cleaning_type\", sa.Text, nullable=False, server_default=\"spot_clean\"),\n        sa.Column(\"price\", sa.Numeric(10, 2), nullable=False),\n    )\n```\n\n이라는 함수를 새로 작성해서 진행했다. **`sqlmodel`** 을 사용하는 만큼, 임시로 모델을 생성하고, 그 모델에서 자동으로 생성된 테이블에서 `Column`을 추출하는 방식으로 진행한다. 추후 모델을 정의할 파일 위치를 확인하면 옮기고 수정할 예정.\n\n```bash\n❯ mkdir backend/app/db/models\n❯ touch backend/app/db/models/base.py backend/app/db/models/temp.py\n```\n\n```python\n# touch backend/app/db/models/base.py\nclass base_model(SQLModel):\n    @classmethod\n    def get_table(cls) -> Table:\n        if (table := getattr(cls, \"__table__\", None)) is None:\n            raise ValueError(\"not table\")\n        return table\n```\n\n```python\n# backend/app/db/models/temp.py\nfrom pydantic import condecimal\nfrom sqlmodel import Field\n\nfrom .base import base_model\n\n\nclass cleanings(base_model, table=True):\n    id: int | None = Field(None, primary_key=True)\n    name: str = Field(index=True)\n    description: str | None = None\n    cleaning_type: str = Field(\n        (_default_cleaning_type := \"spot_clean\"),\n        sa_column_kwargs={\"server_default\": _default_cleaning_type},\n    )\n    price: condecimal(max_digits=10, decimal_places=2)  # type: ignore\n```\n\n이 튜토리얼은 청소 관련 주제로 작성되기에, 테이블 이름이 `cleanings`이다.\n\n> - `id`: 각 항목에 대한 고유한 식별값.\n> - `name`: 해당 항목에 대한 이름. `index=True` 옵션으로 인해 더 빠른 조회가 가능하다.\n> - `description`: 해당 항목에 대한 설명이지만, `null`값(파이썬에서는 `None`값)이 가능하다.\n> - `cleaning_type`: 해당 항목의 타입\n> - `price`: 해당 항목의 가격\n\n모델을 정의했으니, 이제 예제를 따라 함수를 정의한다.\n\n```python\n# backend/app/db/migrations/versions/f721febf752b_create_account_table.py\n(...)\n\ndef create_cleanings_table() -> None:\n    import sys\n    from pathlib import Path\n    sys.path.append(Path(__file__).resolve().parents[4].as_posix())\n    from app.db.models.temp import cleanings\n\n    table = cleanings.get_table()\n\n    op.create_table(\n        table.name,\n        *table.columns\n    )\n\ndef upgrade():\n    create_cleanings_table()\n\n\ndef downgrade():\n    op.drop_table('cleanings')\n```\n\n대부분의 경우 **`sqlmodel`** 의 `Field`의 변수로 가능하고, `server_default`에 대해서만 따로 `sa_column_kwargs`로 처리했다. 이렇게 하지 않아도 쿼리에 정상적으로 기본값이 적용되는 것으로 알고 있지만, 혹시 몰라서..\n\n이제 마이그레이션을 진행한다.\n\n```bash\nroot@9c425f594efa:/backend# alembic upgrade head\nINFO  [alembic.env] Running migrations online\nINFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\nINFO  [alembic.runtime.migration] Running upgrade  -> f721febf752b, create account table\n```\n\n테이블이 정상적으로 생성됐는지, `db` 컨테이너에서 **`psql`** 을 이용해 확인해본다.\n\n```bash\nbash-5.1# psql -h localhost -U postgres --dbname=postgres\npsql (14.2)\nType \"help\" for help.\n\npostgres=# select * from cleanings;\n id | name | description | cleaning_type | price\n----+------+-------------+---------------+-------\n(0 rows)\n```\n\n정상적으로 생성 된 것을 확인했다.\n","mtime":"2022-08-13T00:12:20.000+09:00","href":"velog/fastapi 튜토리얼 -2- sql 서버 연결","data":{"title":"fastapi 튜토리얼 -2- sql 서버 연결","date":"2022-04-28T19:51:57.390+09:00","tags":["fastapi","alembic","postgres","python","@all"],"page":"fastapi 튜토리얼","summary":"fastapi 사용법을 다시 공부할겸, 참고할만한 좋은 예제가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다."}}]},"__N_SSG":true}