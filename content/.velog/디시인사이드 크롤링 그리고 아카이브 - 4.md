---
title: '디시인사이드 크롤링 그리고 아카이브 - 4'
description: '게시글 내용을 가져올 때, 수정된 경우에만 가져오려고 했는데,생각해보니 수정됐는지 아닌지를 알 방법이 없다는 생각이 들었습니다.그냥 다 긁어야겠네요..어차피 최종 목표는첫페이지만 가져오기5분 간격 스케쥴 돌리기삭제된 게시글 복구하기이렇게인데,첫 페이지만 5분 간격으로 '
date: 2022-01-07T19:10:46.839Z
tags: ['python', 'scrapy']
hide: true
---

게시글 내용을 가져올 때, 수정된 경우에만 가져오려고 했는데,
생각해보니 수정됐는지 아닌지를 알 방법이 없다는 생각이 들었습니다.
그냥 다 긁어야겠네요..

어차피 최종 목표는
첫페이지만 가져오기
5분 간격 스케쥴 돌리기
삭제된 게시글 복구하기
이렇게인데,
첫 페이지만 5분 간격으로 가져오는건 부담이 아닐거라 생각되니 그냥 다 긁어도 될거같습니다.

한 페이지에 게시글 수라고 해봐야 50건쯤 될테니까요

그리고 기존의 title_pipe에서 그냥 다 처리하면 될거같은데
scrapy를 연습해보는 의미로 시작한 것이기도 하니,
그냥 post_pipe, reply_pipe를 새로 생성해서 진행하겠습니다.

이제보니 게시글을 확인할때, 정확한 일자와 정확한 시간이 제공되네요
title에서 날짜를 굳이 조회할 필요는 없어보입니다.
유저 정보도 그냥 게시글을 가져올때 같이 가져오는걸로 변경합니다.
전부다 가져올거니 차라리 이런건 마음이 편하네요

사용할 모델을 새롭게 정의합니다.

```python
from datetime import datetime
from typing import Optional

from sqlmodel import Field, Relationship, SQLModel

__all__ = ("title", "post", "reply")


class base(SQLModel):
    user_name: str = Field(min_length=1)
    user_ip: Optional[str] = Field(None, min_length=1)
    date: datetime
    text: str


class title(SQLModel, table=True):
    id: int = Field(index=True, primary_key=True, sa_column_kwargs={"unique": True})
    text: str
    reply_count: int = Field(ge=0)
    link: str = Field(min_length=1)


class post(base, table=True):
    id: int = Field(index=True, primary_key=True, sa_column_kwargs={"unique": True})
    title_id: int = Field(foreign_key="title.id")

    reply_list: list["reply"] = Relationship(back_populates="post")


class reply(base, table=True):
    id: Optional[int] = Field(None, primary_key=True)
    mother_reply_id: Optional[int] = Field(None, gt=0)

    post: post = Relationship(back_populates="reply_list")
```

이에 맞춰서 spider도 수정합니다.
spider는 기존의 title을 반환하고,
이후 post와 reply를 반환하게 합니다.

```python
# app/scripts/crawl/crawl/items.py
from ...model import post, reply, title

__all__ = ("title", "post", "reply")
```

......
하다보니
혹시 글에 이미지가 있으면 이거도 따로 관리를 해야한다는 생각이 들었습니다.
그러려니 좀 귀찮아졌어요.

그런데 어차피 전부 다 크롤링 하는건데
그럼 그냥 페이지를 이미지로 만들어서 저장하는게 편하겠다는 생각이 들었습니다.

모델부터 새로 짭니다.
spider도 이전에 했는건 그다지 쓸모없어보이니
새로짜야겠네요
다만 훨씬 깔끔해지긴 할거같아요.

하는김에 startproject대신 그냥 직접 관리해야겠습니다.
어차피 scrapy가 아닌 app.py로 실행하는데
굳이? 라는 생각이 들었습니다.

생각이 좀 정리되면 다시 글쓰는거로..
