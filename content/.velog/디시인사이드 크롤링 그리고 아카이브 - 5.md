---
title: '디시인사이드 크롤링 그리고 아카이브 - 5'
description: '그냥 생각없이 이미지로 저장해버리니 엄청 편합니다이미지로 변환하기 위해 라이브러리와 프로그램을 설치해야합니다.모델은 이렇게 했습니다.crawl은 이제 spider와 pipeline 스크립트만 남겨놨습니다.마지막으로 app을 수정합니다.디시인사이드에서 차단을 하는것 같아'
date: 2022-01-07T20:18:45.397Z
tags: ['python', 'scrapy']
hide: true
---

그냥 생각없이 이미지로 저장해버리니 엄청 편합니다

이미지로 변환하기 위해 라이브러리와 프로그램을 설치해야합니다.

```console
poetry add imgkit
sudo pacman -S wkhtmltopdf
```

모델은 이렇게 했습니다.

```python
# app/scripts/model/__init__.py
from datetime import datetime

from sqlmodel import Field, SQLModel

__all__ = ("post",)


class post(SQLModel, table=True):
    id: int = Field(index=True, primary_key=True, sa_column_kwargs={"unique": True})
    date: datetime
    link: str = Field(min_length=1)
    image: bytes = Field(min_length=1)
```

crawl은 이제 spider와 pipeline 스크립트만 남겨놨습니다.

```python
# app/scripts/crawl/spider.py
import logging
from datetime import datetime
from typing import Generator, Union
from urllib.parse import parse_qs

import imgkit
import scrapy as sc
from scrapy.http.response.html import HtmlResponse
from scrapy.selector import Selector, SelectorList

from ..model import post

__all__ = ("git_spider",)
logger = logging.getLogger(__name__)


def check_subject(title: Selector) -> bool:
    try:
        _subject: Selector = title.css("td.gall_subject")[0]  # type: ignore
    except KeyError:
        return False
    _subject_text: Union[str, None] = _subject.css("td::text").get(None)

    if not _subject_text:
        return False

    return True


class git_spider(sc.Spider):
    name: str = "git"
    start_urls: list[str] = ["https://gall.dcinside.com/mgallery/board/lists?id=github"]
    _date_format = "%Y-%m-%d %H:%M:%S"

    def parse(self, response: HtmlResponse) -> Generator[sc.Request, None, None]:
        for title in response.css("tr.ub-content.us-post"):
            title: Selector
            if not check_subject(title):
                continue

            link: Union[str, None] = title.css(
                "td.gall_tit.ub-word > a::attr(href)"
            ).get(None)
            if not link:
                continue

            yield response.follow(link, self.parse_post)

    def parse_post(self, response: HtmlResponse) -> Generator[post, None, None]:
        _id: int = int(parse_qs(response.url)["no"][0])

        for_post_user_area: SelectorList = response.css("div.fl")  # type: ignore

        post_user_name: Union[str, None] = for_post_user_area.css(
            "span.nickname::attr(title)"
        ).get(None)
        if not post_user_name:
            raise ValueError("nickname이 없는 경우가 왜 생기지?")

        post_user_ip: Union[str, None] = for_post_user_area.css("span.ip::text").get(
            None
        )
        if post_user_ip:
            post_user_ip = post_user_ip.strip("()")

        _date: Union[str, None] = for_post_user_area.css(
            "span.gall_date::attr(title)"
        ).get(None)
        if not _date:
            raise ValueError("date가 없는 경우가 왜 생기지?")
        date: datetime = datetime.strptime(_date, self._date_format)

        image: bytes = imgkit.from_url(response.url, False)

        logger.info(f"{_id=}, {post_user_name=}, {response.url=}")
        yield post(id=_id, date=date, link=response.url, image=image)
```

```python
# app/scripts/crawl/pipelines.py
from asyncio import gather, get_event_loop
from typing import Any, Coroutine

import scrapy as sc
from sqlmodel.ext.asyncio.session import AsyncSession

from ..database import create_model_table, get_async_session
from ..model import post


class post_pipe:
    def __init__(self) -> None:
        self.loop = get_event_loop()
        self.coro_list: list[Coroutine[Any, Any, None]] = []

    def close_spider(self, spider: sc.Spider) -> None:
        gather_coro = gather(*self.coro_list)
        self.loop.run_until_complete(gather_coro)

    def open_spider(self, spider: sc.Spider) -> None:
        init_engine_coro = create_model_table()
        self.loop.run_until_complete(init_engine_coro)

    def process_item(self, item: post, spider: sc.Spider) -> None:
        result_coro = self._process_item(item, spider)
        self.coro_list.append(result_coro)

    async def _process_item(self, item: post, spider: sc.Spider) -> None:
        async with get_async_session() as session:
            data = await session.get(post, item.id)
            if data:
                await self._update_title(session, data=data, item=item)
            else:
                await self._create_title(session, item=item)

    async def _create_title(self, session: AsyncSession, /, item: post) -> post:
        session.add(item)
        await session.commit()
        await session.refresh(item)
        return item

    async def _update_title(
        self, session: AsyncSession, /, data: post, item: post
    ) -> post:
        item_dict = item.dict(exclude_unset=True)
        for key, value in item_dict.items():
            setattr(data, key, value)

        session.add(data)
        await session.commit()
        await session.refresh(data)
        return data
```

마지막으로 app을 수정합니다.

```python
# app/app.py
from scrapy.crawler import CrawlerProcess

from scripts.crawl.pipelines import post_pipe
from scripts.crawl.spider import git_spider


def main() -> None:
    process = CrawlerProcess(
        settings=dict(
            ROBOTSTXT_OBEY=True,
            ITEM_PIPELINES={
                post_pipe: 300,
            },
            DOWNLOAD_DELAY=1,
            CONCURRENT_REQUESTS=4,
            LOG_LEVEL="INFO",
        )
    )
    process.crawl(git_spider)
    process.start()


if __name__ == "__main__":
    main()
```

디시인사이드에서 차단을 하는것 같아서 딜레이와 병렬 제한을 추가했습니다.

만족스러운 속도는 아니지만, 어쨌든 이미지로 잘 변환돼서 db에 저장된 것을 확인했습니다.

이제 삭제된 글을 탐색하고,
삭제된 글을 복구하는 기능을 추가하면 됩니다.
특정한 기간이 지난 글을 db에서 제거도 해주면 좋을거 같습니다.

음..
파이프라인에서 코루틴을 모아서 한번에 처리하게 했더니
메모리에 이미지가 남아있는게 마음에 안듭니다
어차피 이미지로 변환하느라 느려졌는데
db에 넣을때 비동기를 고집할 필요가 없을거 같네요
드라이버를 고치기는 귀찮고
gather를 쓰지 않는쪽으로 수정합니다

기왕 하는김에 loguru도 설치합니다

```console
poetry add loguru
```

```python
# app/scripts/crawl/pipelines.py
from asyncio import get_event_loop

import scrapy as sc
from loguru import logger
from sqlmodel.ext.asyncio.session import AsyncSession

from ..database import create_model_table, get_async_session
from ..model import post

__all__ = ("post_pipe",)


class post_pipe:
    def __init__(self) -> None:
        self.loop = get_event_loop()
        # self.coro_list: list[Coroutine[Any, Any, None]] = []

    # def close_spider(self, spider: sc.Spider) -> None:
    #     gather_coro = gather(*self.coro_list)
    #     self.loop.run_until_complete(gather_coro)

    def open_spider(self, spider: sc.Spider) -> None:
        init_engine_coro = create_model_table()
        self.loop.run_until_complete(init_engine_coro)

    def process_item(self, item: post, spider: sc.Spider) -> None:
        result_coro = self._process_item(item, spider)
        self.loop.run_until_complete(result_coro)
        # self.coro_list.append(result_coro)

    async def _process_item(self, item: post, spider: sc.Spider) -> None:
        async with get_async_session() as session:
            data = await session.get(post, item.id)
            if data:
                result = await self._update_title(session, data=data, item=item)
            else:
                result = await self._create_title(session, item=item)
            logger.debug(f"database<- {result.id=}, {result.date=}, {result.link=}")

    async def _create_title(self, session: AsyncSession, /, item: post) -> post:
        session.add(item)
        await session.commit()
        await session.refresh(item)
        return item

    async def _update_title(
        self, session: AsyncSession, /, data: post, item: post
    ) -> post:
        item_dict = item.dict(exclude_unset=True)
        for key, value in item_dict.items():
            setattr(data, key, value)

        session.add(data)
        await session.commit()
        await session.refresh(data)
        return data
```

spider.py도 logger를 loguru.logger로 변경했습니다.

파이프라인 작동 방식을 변경하니 아까보다 속도가 개선됐습니다.
