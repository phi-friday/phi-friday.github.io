---
title: '디시인사이드 크롤링 그리고 아카이브 - 3'
description: '이전 글에서 말한 것 처럼, item pipeline을 활용해서, json대신 db에 저장할 생각입니다. 그전에 이전처럼 dict로 관리하지 말고, item을 sqlmodel로 정의하겠습니다. sqlmodel을 사용하기 위해 itemadapter를 설치합니다.  sql'
date: 2022-01-07T16:30:30.532Z
tags: ['python', 'scrapy']
hide: true
---

이전 글에서 말한 것 처럼, item pipeline을 활용해서, json대신 db에 저장할 생각입니다.
그전에 이전처럼 dict로 관리하지 말고,
item을 sqlmodel로 정의하겠습니다.

sqlmodel의 모델을 scrapy에서 사용할 수 있도록 하는 라이브러리를 설치합니다.

```console
poetry add itemadapter
```

sql로 사용할 모델이니 crawl보다는 model로 관리하는게 맞다는 생각이 들어서, model경로에 title클래스를 정의합니다.

```python
# app/scripts/model/__init__.py
from datetime import datetime

from sqlmodel import Field, SQLModel

__all__ = ("title",)


class title(SQLModel, table=True):
    id: int = Field(index=True, primary_key=True, sa_column_kwargs={"unique": True})
    date: datetime
    username: str = Field(min_length=1)
    id_or_ip: str = Field(min_length=1)
    text: str
    reply: int = Field(ge=0)
    link: str = Field(min_length=1)
```

(혹시나 제목이 공란인 경우가 있을 수도 있겠다 싶어서 text에는 조건을 추가하지 않았습니다)

이전에 깜박하고 추가하지 않았던, db 초기화 작업 함수도 작성합니다.

```python
# api/scripts/database/__init__.py
(전략)

from sqlmodel import SQLModel

(중략)

__all__ = ("get_async_session", "create_model_table")

(중략)

async def create_model_table() -> None:
    async with engine.begin() as conn:
        await conn.run_sync(SQLModel.metadata.create_all)
```

itemloader도 사용해보고 싶지만, 아쉽게도 여기서 제공하는 add_values, replace_values 메소드는 pydantic의 속성 값을 변경하지 않습니다. 그러니 사용하지 않습니다.

scrapy에서 사용하기 위한 파이프라인을 생성합니다.
sqlmodel로 작성된 모델을 db에 저장하는 과정입니다.

```python
# api/scripts/crawl/crawl/items.py
from ...model import title

__all__ = ("title",)
```

```python
# api/scripts/crawl/crawl/pipelines.py
from asyncio import gather, get_event_loop
from typing import Any, Coroutine

import scrapy as sc
from sqlmodel.ext.asyncio.session import AsyncSession

from ...database import create_model_table, get_async_session
from .items import title


class title_pipe:
    def __init__(self) -> None:
        self.loop = get_event_loop()
        self.coro_list: list[Coroutine[Any, Any, title]] = []

    def open_spider(self, spider: sc.Spider) -> None:
        init_engine_coro = create_model_table()
        self.loop.run_until_complete(init_engine_coro)

    def close_spider(self, spider: sc.Spider) -> None:
        gather_coro = gather(*self.coro_list)
        self.loop.run_until_complete(gather_coro)

    def process_item(self, item: title, spider: sc.Spider) -> title:
        result_coro = self._process_item(item, spider)
        self.coro_list.append(result_coro)
        return item

    async def _process_item(self, item: title, spider: sc.Spider) -> title:
        async with get_async_session() as session:
            data = await session.get(title, item.id)
            if data:
                result = await self._update_title(
                    session, data=data, item=item, spider=spider
                )
            else:
                result = await self._create_title(session, item=item, spider=spider)
        return result

    async def _create_title(
        self, session: AsyncSession, /, item: title, spider: sc.Spider
    ) -> title:
        session.add(item)
        await session.commit()
        await session.refresh(item)
        return item

    async def _update_title(
        self, session: AsyncSession, /, data: title, item: title, spider: sc.Spider
    ) -> title:
        item_dict = item.dict(exclude_unset=True)
        for key, value in item_dict.items():
            setattr(data, key, value)

        session.add(data)
        await session.commit()
        await session.refresh(data)
        return data
```

아쉽게도 scrapy는 제한적으로 비동기를 지원하기 때문에,
테이블 초기화와 데이터 삽입은 gather로 모아서 진행합니다..
나중에 응용해서 사용하기 좀 까다롭긴 하겠지만,
제가 제대로 못써서 이러는 걸 수도 있고(사실 가능성이 가장 높죠)
나중에 제대로 지원이 될 수 있으니 일단 이렇게 놔둡니다..

그리고 원래 dict를 반환했던 spider가 title을 반환하도록 수정합니다.

```python
# api/scripts/crawl/crawl/spider/git.py
from datetime import datetime
from typing import Generator, Union

import scrapy as sc
from scrapy.http.response.html import HtmlResponse
from scrapy.selector import Selector, SelectorList

from ..items import title

__all__ = ("git_spider",)


class git_spider(sc.Spider):
    name: str = "git"
    start_urls: list[str] = ["https://gall.dcinside.com/mgallery/board/lists?id=github"]

    def parse(self, response: HtmlResponse) -> Generator[title, None, None]:
        for response_title in response.css("tr.ub-content.us-post"):
            response_title: Selector

            try:
                _subject: Selector = response_title.css("td.gall_subject")[0]  # type: ignore
            except KeyError:
                continue
            _subject_text: Union[str, None] = _subject.css("td::text").get(None)

            if not _subject_text:
                continue

            _id: Union[str, None] = response_title.css("td.gall_num::text").get(None)
            _date: Union[str, None] = response_title.css("td.gall_date::text").get(None)

            if not _id or not _date:
                continue

            try:
                _date_object: datetime = datetime.strptime(_date, "%H:%M")
            except ValueError:
                try:
                    _date_object = datetime.strptime(_date, "%m.%d")
                except ValueError:
                    _date_object = datetime.strptime(_date, "%y.%m.%d")
                else:
                    today = datetime.today()
                    _date_object = _date_object.replace(year=today.year)
            else:
                today = datetime.today()
                _date_object = _date_object.replace(
                    year=today.year, month=today.month, day=today.day
                )

            _user_dict: dict[str, str] = response_title.css(
                "td.gall_writer.ub-writer"
            ).attrib
            _user_name: str = _user_dict["data-nick"]
            _user_id_or_ip: str = _user_dict["data-uid"] or _user_dict["data-ip"]

            if not _user_name or not _user_id_or_ip:
                continue

            _title_sub_select: SelectorList = response_title.css("td.gall_tit.ub-word")  # type: ignore
            _title: Union[str, None] = _title_sub_select.css("a::text").get(None)
            _link: Union[str, None] = _title_sub_select.css("a::attr(href)").get(None)
            _reply_count: Union[str, None] = _title_sub_select.css("span::text").get(
                None
            )

            if not _title or not _link:
                continue
            _link = response.urljoin(_link)

            if not _reply_count:
                _reply_count = "0"
            _reply_count = _reply_count.strip("[]")

            yield title(
                id=int(_id),
                date=_date_object,
                username=_user_name,
                id_or_ip=_user_id_or_ip,
                text=_title,
                reply=int(_reply_count),
                link=_link,
            )
```

(동시에 일자 변경 관련해서 미흡했던 부분을 임시로 돌려막기 합니다...)

끝으로 이제 파이썬 스크립트 내부에서 scrapy를 실행할 수 있게 간단한 스크립트를 하나 작성합니다.

```python
# api/app.py
from scrapy.crawler import CrawlerProcess

from scripts.crawl.crawl.pipelines import title_pipe
from scripts.crawl.crawl.spiders.git import git_spider


def main() -> None:
    process = CrawlerProcess(
        settings=dict(
            ROBOTSTXT_OBEY=True,
            ITEM_PIPELINES={
                title_pipe: 300,
            },
        )
    )
    process.crawl(git_spider)
    process.start()


if __name__ == "__main__":
    main()
```

이제 최상위 폴더에서 다음과 같이 실행해보면

```console
python ./app.py
```

![](/images/7213d078-d969-41c9-80ef-6b185d2e95ef-2022-01-08_01-26.png)
위와 같이 정상적으로 게시글이 db에 저장된 것을 확인할 수 있습니다.

이제 다음 목표는,
각 게시글의 링크에서 게시글 내용과 댓글을 가져와서 저장하는겁니다.
이때 게시글을 조회하는 조건은, 처음 발견한 게시글이거나,
댓글 수에 변화가 있을때로 한정할 생각입니다.

현재 디렉토리 구성입니다

```console
.
├── app
│   ├── app.py
│   ├── dcgit.db
│   └── scripts
│       ├── base
│       ├── config
│       │   ├── __init__.py
│       │   ├── database.json
│       │   └── default.py
│       ├── crawl
│       │   ├── crawl
│       │   │   ├── __init__.py
│       │   │   ├── items.py
│       │   │   ├── middlewares.py
│       │   │   ├── pipelines.py
│       │   │   ├── settings.py
│       │   │   └── spiders
│       │   │       ├── __init__.py
│       │   │       └── git.py
│       │   └── scrapy.cfg
│       ├── database
│       │   └── __init__.py
│       └── model
│           └── __init__.py
├── poetry.lock
└── pyproject.toml
```
