---
title: '디시인사이드 크롤링 그리고 아카이브 - 1'
description: '크롤링 관련 연습을 해보고 싶기도 했고 질문글을 올리고 답변을 얻으면 삭제하는 모습이 보기 싫어서 디시인사이드 특정 갤러리의 글을 주기적으로 모니터링해서, 새로운 글/댓글이 작성/갱신되면 따로 보관하고, 글이 삭제되면 해당 글을 복구하고자 한다.  이전 글과 다르게, '
date: 2022-01-05T15:04:05.707Z
tags: ['python', 'scrapy']
hide: true
---

크롤링 관련 연습을 해보고 싶기도 했고
질문글을 올리고 답변을 얻으면 삭제하는 모습이 보기 싫어서
디시인사이드 특정 갤러리의 글을 주기적으로 모니터링해서,
새로운 글/댓글이 작성/갱신되면 따로 보관하고,
글이 삭제되면 해당 글을 복구하고자 한다.

이전 글과 다르게, 따로 해본적 없는 일이기도 하고
조금씩 해가면서 같이 글을 쓸 예정이기에,
이전글보다 훨씬 두서없고 정리되지 않을 예정이기에, 주의할 것.
특히, 오개념이 많을 수 있으니 주의할 것.

우선 가상 환경 설정 후 scrapy 설치

```console
mkdir dcgit && cd dcgit
git init
pyenv virtualenv 3.9.9 dcgit
pyenv local dcgit
poetry init
poetry add scrapy
```

현재 생각으로는,
적당한 db에 일정 기간동안 글을 보존하고
보존 기간 중 글삭튀가 발생할 경우 글을 복구할 예정이므로
이전에 사용한 적이 있는 sqlmodel을 사용
그리고 당장은 sqlite를 사용할 예정이므로, sqlite의 비동기 라이브러리 aiosqlite를 함께 설치

```console
poetry add sqlmodel
poetry add --dev aiosqlite
```

json을 좀 더 빠르게 파싱하기 위한 orjson 설치
주피터로 간단하게 확인해보기 위한 ipykernel설치
코드 포맷팅을 위한 black, isort 설치

```console
poetry add orjson
poetry add --dev ipykernel black isort
```

당장은 이정도면 될 것 같다.

디렉토리 구성은 우선 최상위 폴더에 app폴더를 생성하고
app/scripts폴더를 생성한 다음
app/scripts/crawl폴더를 생성해서
해당 폴더에서 scrapy를 사용한 작업을 진행할 예정
app/scripts에는 crawl만이 아닌 db나 설정 관련 등 다른 모듈이 함께 작성될 곳.

db정보는 json형태로 app/scripts/config에 저장

```json
{
  "drivername": "sqlite+aiosqlite",
  "username": null,
  "password": null,
  "host": "dcgit.db",
  "port": null,
  "database": "archive",
  "query": {}
}
```

간단하게 작성한 설정 모듈

```python
# app/scripts/config/default.py
from pathlib import Path
from typing import Any, Optional

from pydantic import BaseSettings, Field

_here = Path(__file__).resolve().parent


def _get_database() -> "_database":
    return _database.parse_file(
        _here / "database.json", content_type="json", encoding="utf-8"
    )


def _get_root_path() -> Path:
    return _here.parent.parent


class _database(BaseSettings):
    drivername: str
    host: Optional[str] = None
    username: Optional[str] = None
    password: Optional[str] = None
    port: Optional[str] = None
    database: Optional[str] = None
    query: dict[str, Any] = Field(default_factory=dict)


class _settings(BaseSettings):
    root_path: Path = Field(default_factory=_get_root_path)
    database: _database = Field(default_factory=_get_database)

    class Config(BaseSettings.Config):
        allow_mutation = False
        arbitrary_types_allowed = True
```

```python
# app/scripts/config/__init__.py
from .default import _settings

__all__ = ("settings",)
settings = _settings()
```

간단하게 작성한 db접속용 모듈

```python
from contextlib import asynccontextmanager
from typing import AsyncGenerator

from sqlalchemy.engine.url import URL
from sqlalchemy.ext.asyncio.engine import create_async_engine
from sqlmodel.ext.asyncio.session import AsyncSession

from ..config import settings

__all__ = ("get_async_session",)

url = URL.create(**settings.database.dict())
engine = create_async_engine(url)


@asynccontextmanager
async def get_async_session() -> AsyncGenerator[AsyncSession, None]:
    async with AsyncSession(engine, autocommit=False, autoflush=False) as session:
        yield session
```

현재 디렉토리 구성

```console
.
├── app
│   └── scripts
│       ├── base
│       ├── config
│       │   ├── __init__.py
│       │   ├── database.json
│       │   └── default.py
│       ├── crawl
│       ├── database
│       │   └── __init__.py
│       └── model
├── poetry.lock
└── pyproject.toml
```

간단하게 확인해본 결과
![](/images/22e5ff38-22bb-4800-bc39-cb278b334010-2022-01-06_00-07.png)
문제없이 실행되는 것을 확인할 수 있다.
(여기서 async, await은 주피터에서 사용하는 loop에서 돌아가므로,
pylance는 오류를 나타내지만,
마치 글로벌에서 사용하듯이 사용할 수 있다.)

다음번에 scrapy를 사용해보는거로..
