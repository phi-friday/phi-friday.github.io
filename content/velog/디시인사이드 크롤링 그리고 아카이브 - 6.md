---
title: '디시인사이드 크롤링 그리고 아카이브 - 6'
description: '하다보니 댓글을 삭제하고 도망치는 경우에 대해서는 생각을 못했네요이건 일단 나중에 생각하기로 하고..해당 글이 삭제됐는지 아닌지 확인하려면 어떻게 해야할까요일단 지금 생각중인 크롤 대상은 딱 1페이지에 있는 글입니다.1페이지의 글 번호의 최소값보다 큰 번호에 대해 db'
date: 2022-01-08T13:02:51.268Z
tags: ['python', 'scrapy']
hide: true
---

하다보니 댓글을 삭제하고 도망치는 경우에 대해서는 생각을 못했네요
이건 일단 나중에 생각하기로 하고..

해당 글이 삭제됐는지 아닌지 확인하려면 어떻게 해야할까요
일단 지금 생각중인 크롤 대상은 딱 1페이지에 있는 글입니다.
1페이지의 글 번호의 최소값보다 큰 번호에 대해 db에서 조회했을 때,
현재 1페이지의 번호에는 없는데 db에는 있는 글이 있다면 복구하면 될 것 같습니다.

복구할때 유저 정보랑 제목이 필요한데
이미지로 할거라고 그냥 모델에서 빼버렸네요
다시 추가합니다.

```python
# app/scripts/model/__init__.py
from datetime import datetime
from typing import Optional

from sqlmodel import Field, SQLModel

__all__ = ("post",)


class post(SQLModel, table=True):
    id: int = Field(index=True, primary_key=True, sa_column_kwargs={"unique": True})
    title: str
    user_name: str
    user_ip: Optional[str] = None
    date: datetime
    link: str = Field(min_length=1)
    image: bytes = Field(min_length=1)
```

```python
# app/scripts/crawl/spider.py
(전략)

    def parse_post(self, response: HtmlResponse) -> Generator[post, None, None]:
        _id: int = int(parse_qs(response.url)["no"][0])

        title: Union[str, None] = response.css("span.title_subject::text").get(None)
        if title is None:
            raise ValueError("title이 없는 경우가 왜 생기지?")

        for_post_user_area: SelectorList = response.css("div.fl")  # type: ignore

        post_user_name: Union[str, None] = for_post_user_area.css(
            "span.nickname::attr(title)"
        ).get(None)
        if not post_user_name:
            raise ValueError("nickname이 없는 경우가 왜 생기지?")

        post_user_ip: Union[str, None] = for_post_user_area.css("span.ip::text").get(
            None
        )
        if post_user_ip:
            post_user_ip = post_user_ip.strip("()")

        _date: Union[str, None] = for_post_user_area.css(
            "span.gall_date::attr(title)"
        ).get(None)
        if not _date:
            raise ValueError("date가 없는 경우가 왜 생기지?")
        date: datetime = datetime.strptime(_date, self._date_format)

        image: bytes = imgkit.from_url(response.url, False)

        logger.info(f"{_id=}, {post_user_name=}, {response.url=}")
        yield post(
            id=_id,
            title=title,
            user_name=post_user_name,
            user_ip=post_user_ip,
            date=date,
            link=response.url,
            image=image,
        )
```

이런 글이 있는지 없는지 확인하는 파이프라인을 추가해봅니다.

```python
# app/scripts/crawl/pipelines.py
import asyncio
from abc import ABCMeta, abstractmethod
from typing import Iterable, TypeVar, Union

import scrapy as sc
import sqlmodel as sm
from loguru import logger
from sqlmodel.ext.asyncio.session import AsyncSession

from ..database import create_model_table, get_async_session
from ..model import post

__all__ = ("post_pipe",)

_T = TypeVar("_T")


class base_pipe(metaclass=ABCMeta):
    _init_table: bool = False

    def open_spider(self, spider: sc.Spider) -> None:
        if not base_pipe._init_table:
            init_engine_coro = create_model_table()
            loop = asyncio.get_event_loop()
            loop.run_until_complete(init_engine_coro)
            base_pipe._init_table = True

    def process_item(self, item: _T, spider: sc.Spider) -> Union[_T, None]:
        loop = asyncio.get_event_loop()
        result_coro = self._process_item(item, spider)
        return loop.run_until_complete(result_coro)
        # self.coro_list.append(result_coro)

    @abstractmethod
    async def _process_item(self, item: _T, spider: sc.Spider) -> Union[_T, None]:
        ...


class post_pipe(base_pipe):
    async def _process_item(self, item: post, spider: sc.Spider) -> post:
        async with get_async_session() as session:
            data = await session.get(post, item.id)
            if data:
                result = await self._update_title(session, data=data, item=item)
            else:
                result = await self._create_title(session, item=item)
            logger.debug(f"database<- {result.id=}, {result.date=}, {result.link=}")
        return result

    async def _create_title(self, session: AsyncSession, /, item: post) -> post:
        session.add(item)
        await session.commit()
        await session.refresh(item)
        return item

    async def _update_title(
        self, session: AsyncSession, /, data: post, item: post
    ) -> post:
        item_dict = item.dict(exclude_unset=True)
        for key, value in item_dict.items():
            setattr(data, key, value)

        session.add(data)
        await session.commit()
        await session.refresh(data)
        return data


class delete_pipe(base_pipe):
    def __init__(self):
        self.post_id_list: list[int] = []
        self.re_post_count: int = 0
        self.del_post_count: int = 0
        super().__init__()

    def close_spider(self, spider: sc.Spider) -> None:
        loop = asyncio.get_event_loop()
        post_coro = self._find_post()
        loop.run_until_complete(post_coro)
        logger.debug(f"{self.re_post_count=}, {self.del_post_count=}")

    async def _process_item(self, item: post, spider: sc.Spider) -> None:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, self.post_id_list.append, item.id)

    async def _find_post(self) -> None:
        async with get_async_session() as session:
            await asyncio.gather(
                self._find_upper_post(session), self._find_lower_post(session)
            )

    async def _find_upper_post(self, session: AsyncSession, /) -> None:
        min_id = min(self.post_id_list)
        statement = sm.select(post).where(post.id >= min_id)
        post_list: Iterable[post] = await session.exec(statement)  # type: ignore
        post_list = [_post for _post in post_list if _post.id not in self.post_id_list]
        re_post_coro_list = [self._re_post_as_image(_post) for _post in post_list]
        re_post_coro = asyncio.gather(*re_post_coro_list)
        await re_post_coro

    async def _re_post_as_image(self, item: post) -> None:
        await asyncio.sleep(1)
        logger.debug(f"re_post {item.user_name=}, {item.user_ip=}, {item.title=}")
        self.re_post_count += 1

    async def _find_lower_post(self, session: AsyncSession, /):
        min_id = min(self.post_id_list)
        statement = sm.select(post).where(post.id < min_id)
        post_list: Iterable[post] = await session.exec(statement)  # type: ignore
        del_post_coro = self._del_old_post(post_list)
        await del_post_coro

    async def _del_old_post(self, item_list: Iterable[post]) -> None:
        async with get_async_session() as session:
            for item in item_list:
                _item = await session.get(post, item.id)
                del item
                if _item:
                    await session.delete(_item)
                    await session.commit()
                    logger.debug(
                        f"del_post {_item.user_name=}, {_item.user_ip=}, {_item.title=}"
                    )
                    self.del_post_count += 1
                del _item
```

그리고 어차피 테스트중에는 이미지 변환은 불필요한 과정이니 옵션을 추가합니다

```python
# app/scrtips/config/default.py
(전략)

class _settings(BaseSettings):
    debug: bool = Field(False, env="GIT_CRAWL_DEBUG")
    root_path: Path = Field(default_factory=_get_root_path)
    database: _database = Field(default_factory=_get_database)

    class Config(BaseSettings.Config):
        allow_mutation = False
        arbitrary_types_allowed = True
```

```python
# app/scripts/crawl/spider.py
(전략)

        image: bytes
        if settings.debug:
            image = b"debug"
        else:
            image = imgkit.from_url(response.url, False)

(후략)
```

다음과 같이 실행해보니

```console
GIT_CRAWL_DEBUG=1 python ./app.py
```

```sql
2022-01-08 22:24:11.848 | DEBUG    | scripts.crawl.pipelines:_del_old_post:121 - del_post _item.user_name='ㅇㅇ', _item.user_ip='223.39', _item.title='한국에서 언리얼 요즘버전으로 프로젝트 하는 데 있냐'
2022-01-08 22:24:11.854 | DEBUG    | scripts.crawl.pipelines:_del_old_post:121 - del_post _item.user_name='미쿠쟝사랑해요', _item.user_ip='121.169', _item.title='개발 관련 진로상담 받고싶어요'
2022-01-08 22:24:11.858 | DEBUG    | scripts.crawl.pipelines:_del_old_post:121 - del_post _item.user_name='ㅇㅇ', _item.user_ip='124.56', _item.title='러스트 공부해야할 trait들 뭐있음?'
2022-01-08 22:24:12.839 | DEBUG    | scripts.crawl.pipelines:_re_post_as_image:103 - re_post item.user_name='테스트용', item.user_ip='123.44', item.title='글삭튀 테스트'
2022-01-08 22:24:12.843 | DEBUG    | scripts.crawl.pipelines:close_spider:84 - self.re_post_count=1, self.del_post_count=3
```

이렇게 정상적으로 실행된 것을 확인했습니다.
(사전에 db에 삭제된 상태로 간주할 수 있는 레코드를 하나 추가했습니다.)

이제 이미지를 첨부한 글을 올리는걸 구현해야겠네요.
디시인사이드에서 사용하는 post api를 훔쳐서 사용하던가
셀레니움을 이용해야겠는데,
고민입니다.

그리고 가능하면 작성되는 ip도 숨기고 싶기 때문에,
vpn을 사용할 생각인데,
이거도 매번 달라지게 잘 해봐야겠네요
다행히 이미 구매해서 사용중인 vpn은 있으니, 그걸 잘 활용해야죠..

오늘은 여기까지..
