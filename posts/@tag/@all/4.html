<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta name="author" content="phi.friday@gmail.com, phi"/><meta name="robots" content="index,follow,noarchive"/><meta name="google-site-verification" content="lW107Dj5ageygd67UUzTm-kGls5d-THy9jJQZqLoauw"/><title>phi.log</title><link href="https://use.fontawesome.com/releases/v5.15.1/css/all.css" rel="stylesheet"/><meta name="next-head-count" content="7"/><link rel="preload" href="/_next/static/css/e61452b80f7f8356.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e61452b80f7f8356.css" data-n-g=""/><link rel="preload" href="/_next/static/css/c0b66b64eb886a29.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c0b66b64eb886a29.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/_next/static/chunks/main-5dc3bdee87ff18dd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-f6271bdec05edb1f.js" defer=""></script><script src="/_next/static/chunks/996-f3cf67c2e3ac5e06.js" defer=""></script><script src="/_next/static/chunks/756-1349105dbea71525.js" defer=""></script><script src="/_next/static/chunks/224-ddba219ecdd5c904.js" defer=""></script><script src="/_next/static/chunks/pages/posts/@tag/%5B...tag%5D-5364b6481a3135f4.js" defer=""></script><script src="/_next/static/MQ_lMVQgBI0RkmUA6wvVT/_buildManifest.js" defer=""></script><script src="/_next/static/MQ_lMVQgBI0RkmUA6wvVT/_ssgManifest.js" defer=""></script><script src="/_next/static/MQ_lMVQgBI0RkmUA6wvVT/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><style>
      #nprogress {
        pointer-events: none;
      }
      #nprogress .bar {
        background: #29D;
        position: fixed;
        z-index: 9999;
        top: 0;
        left: 0;
        width: 100%;
        height: 3px;
      }
      #nprogress .peg {
        display: block;
        position: absolute;
        right: 0px;
        width: 100px;
        height: 100%;
        box-shadow: 0 0 10px #29D, 0 0 5px #29D;
        opacity: 1;
        -webkit-transform: rotate(3deg) translate(0px, -4px);
        -ms-transform: rotate(3deg) translate(0px, -4px);
        transform: rotate(3deg) translate(0px, -4px);
      }
      #nprogress .spinner {
        display: block;
        position: fixed;
        z-index: 1031;
        top: 15px;
        right: 15px;
      }
      #nprogress .spinner-icon {
        width: 18px;
        height: 18px;
        box-sizing: border-box;
        border: solid 2px transparent;
        border-top-color: #29D;
        border-left-color: #29D;
        border-radius: 50%;
        -webkit-animation: nprogresss-spinner 400ms linear infinite;
        animation: nprogress-spinner 400ms linear infinite;
      }
      .nprogress-custom-parent {
        overflow: hidden;
        position: relative;
      }
      .nprogress-custom-parent #nprogress .spinner,
      .nprogress-custom-parent #nprogress .bar {
        position: absolute;
      }
      @-webkit-keyframes nprogress-spinner {
        0% {
          -webkit-transform: rotate(0deg);
        }
        100% {
          -webkit-transform: rotate(360deg);
        }
      }
      @keyframes nprogress-spinner {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }
    </style><div class="container" style="max-width:900px;min-width:400px"><div class="container default_app"><header><div class="container"><nav class="navbar-light fixed-top navbar navbar-expand-sm bg-light" role="navigation"><div class="container" style="max-width:900px"><div class="container-fluid"><ul class="navbar-nav w-100"><li class="nav-item"><a data-test="nav-link" class="nav-link" href="/"><div class="text-nowrap nav_text__4OEN1"><i class="fa fa-home"></i> Home</div></a></li><li class="nav-item"><a data-test="nav-link" class="nav-link" href="/posts/@tag"><div class="text-nowrap nav_text__4OEN1"><i class="fa fa-hashtag"></i> Post</div></a></li><li class="nav-item"><a data-test="nav-link" class="nav-link" href="/posts/@page"><div class="text-nowrap nav_text__4OEN1"><i class="fa fa-blog"></i> Page</div></a></li><div class="container d-flex justify-content-end align-self-center"><div class="row"><div class="col"><form class="input-group w-auto" method="get" action="https://www.google.com/search" target="_blank" style="min-width:230px"><input type="hidden" name="sitesearch" value="phi-friday.github.io"/><input type="search" class="form-control" placeholder="Search in Google" aria-label="Search" name="q" maxLength="255"/><button class="ripple ripple-surface btn btn-outline-primary" role="button"><i class="fa fa-search"></i></button></form></div></div></div></ul></div></div></nav></div></header><main><div class="container"><div class="row"><div><span><span class="badge bg-primary mx-1">@all<!-- --> <span class="badge bg-danger">25</span></span></span><span><span class="badge bg-light mx-1 text-dark">#python<!-- --> <span class="badge bg-danger">21</span></span></span><span><span class="badge bg-light mx-1 text-dark">#fastapi<!-- --> <span class="badge bg-danger">14</span></span></span><span><span class="badge bg-light mx-1 text-dark">#sqlmodel<!-- --> <span class="badge bg-danger">8</span></span></span><span><span class="badge bg-light mx-1 text-dark">#fastapi-users<!-- --> <span class="badge bg-danger">4</span></span></span><span><span class="badge bg-light mx-1 text-dark">#crud<!-- --> <span class="badge bg-danger">4</span></span></span><span><span class="badge bg-light mx-1 text-dark">#returns<!-- --> <span class="badge bg-danger">3</span></span></span><span><span class="badge bg-light mx-1 text-dark">#함수형 프로그래밍<!-- --> <span class="badge bg-danger">3</span></span></span><span><span class="badge bg-light mx-1 text-dark">#windows<!-- --> <span class="badge bg-danger">3</span></span></span><span><span class="badge bg-light mx-1 text-dark">#wsl<!-- --> <span class="badge bg-danger">3</span></span></span><span><span class="badge bg-light mx-1 text-dark">#tdd<!-- --> <span class="badge bg-danger">2</span></span></span><span><span class="badge bg-light mx-1 text-dark">#anyio<!-- --> <span class="badge bg-danger">2</span></span></span><span><span class="badge bg-light mx-1 text-dark">#async<!-- --> <span class="badge bg-danger">2</span></span></span><span><span class="badge bg-light mx-1 text-dark">#vim<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#js<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#ts<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#nextjs<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#velog<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#github<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#restful<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#pytest<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#alembic<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#postgres<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#black<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#isort<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#vscode<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#asyncio<!-- --> <span class="badge bg-danger">1</span></span></span><span><span class="badge bg-light mx-1 text-dark">#trio<!-- --> <span class="badge bg-danger">1</span></span></span></div></div><div class="row"><div class="container"><div class="card border border-1 shadow-3 my-3 hover-shadow"><div class="card-header"><div><span><span class="badge bg-light mx-1 text-dark">#fastapi<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#python<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#sqlmodel<!-- --> </span></span></div></div><div class="card-body"><h5 class="card-title">fastapi 튜토리얼 -3- sql 모델 정의</h5><p class="card-subtitle"><p class="card-text text-muted"><h6><time dateTime="2022-04-28T21:51:36.869+09:00">작성일: 2022년 4월 28일</time></h6></p></p><p class="card-text">fastapi 사용법을 다시 공부할겸, 참고할만한 좋은 예제가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다.</p></div></div><div class="card border border-1 shadow-3 my-3 hover-shadow"><div class="card-header"><div><span><span class="badge bg-light mx-1 text-dark">#fastapi<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#alembic<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#postgres<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#python<!-- --> </span></span></div></div><div class="card-body"><h5 class="card-title">fastapi 튜토리얼 -2- sql 서버 연결</h5><p class="card-subtitle"><p class="card-text text-muted"><h6><time dateTime="2022-04-28T19:51:57.390+09:00">작성일: 2022년 4월 28일</time></h6></p></p><p class="card-text">fastapi 사용법을 다시 공부할겸, 참고할만한 좋은 예제가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다.</p></div></div><div class="card border border-1 shadow-3 my-3 hover-shadow"><div class="card-header"><div><span><span class="badge bg-light mx-1 text-dark">#fastapi<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#python<!-- --> </span></span></div></div><div class="card-body"><h5 class="card-title">fastapi 튜토리얼 -1- 초기 설정</h5><p class="card-subtitle"><p class="card-text text-muted"><h6><time dateTime="2022-04-27T23:30:14.239+09:00">작성일: 2022년 4월 27일</time></h6></p></p><p class="card-text">fastapi 사용법을 다시 공부할겸, 참고할만한 좋은 예제가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다.</p></div></div><div class="card border border-1 shadow-3 my-3 hover-shadow"><div class="card-header"><div><span><span class="badge bg-light mx-1 text-dark">#black<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#isort<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#python<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#vscode<!-- --> </span></span></div></div><div class="card-body"><h5 class="card-title">파이썬 코드를 깔끔하게, 그리고 일관성 있게 작성하는법</h5><p class="card-subtitle"><p class="card-text text-muted"><h6><time dateTime="2021-11-29T23:56:34.516+09:00">작성일: 2021년 11월 29일</time></h6></p></p><p class="card-text">black과 isort를 vscode에서 사용하자</p></div></div><div class="card border border-1 shadow-3 my-3 hover-shadow"><div class="card-header"><div><span><span class="badge bg-light mx-1 text-dark">#anyio<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#async<!-- --> </span></span><span><span class="badge bg-light mx-1 text-dark">#python<!-- --> </span></span></div></div><div class="card-body"><h5 class="card-title">파이썬으로 동시성 프로그래밍을 쉽게 하는법 - 2</h5><p class="card-subtitle"><p class="card-text text-muted"><h6><time dateTime="2021-11-29T18:42:54.419+09:00">작성일: 2021년 11월 29일</time></h6></p></p><p class="card-text">anyio로 작업 생성 + 작업간 정보 주고받기</p></div></div></div></div><div class="row"><ul class="pagination justify-content-center"><li class="page-item"><a class="page-link" href="/posts/@tag/@all/1"><i class="fa fa-angle-double-left"></i></a></li><li class="page-item disabled"><a class="page-link" href="/posts/@tag/@all/0"><i class="fa fa-angle-left"></i></a></li><li class="page-item" href="/posts/@tag/@all/1"><a class="page-link">1</a></li><li class="page-item" href="/posts/@tag/@all/2"><a class="page-link">2</a></li><li class="page-item" href="/posts/@tag/@all/3"><a class="page-link">3</a></li><li class="page-item active"><a class="page-link">4</a></li><li class="page-item" href="/posts/@tag/@all/5"><a class="page-link">5</a></li><li class="page-item"><a class="page-link" href="/posts/@tag/@all/6"><i class="fa fa-angle-right"></i></a></li><li class="page-item"><a class="page-link" href="/posts/@tag/@all/6"><i class="fa fa-angle-double-right"></i></a></li></ul></div></div></main><footer class="text-center text-muted"></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"tag":"@all","page":4,"max_page":6,"tag_counter":[["@all",25],["python",21],["fastapi",14],["sqlmodel",8],["fastapi-users",4],["crud",4],["returns",3],["함수형 프로그래밍",3],["windows",3],["wsl",3],["tdd",2],["anyio",2],["async",2],["vim",1],["js",1],["ts",1],["nextjs",1],["velog",1],["github",1],["restful",1],["pytest",1],["alembic",1],["postgres",1],["black",1],["isort",1],["vscode",1],["asyncio",1],["trio",1]],"posts":[{"name":"fastapi 튜토리얼 -3- sql 모델 정의","content":"\n**`fastapi`** 사용법을 다시 공부할겸, 참고할만한 좋은 [예제](https://www.jeffastor.com/blog/populating-cleaning-jobs-with-user-offers-in-fastapi)가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다.\n지난번처럼 어쩌다 그만둘 수도 있긴 하지만...\n\n---\n\n## **cleanings** 모델 생성\n\n### 모델 정의\n\n작성 전 미리 전반적인 내용을 훑어보니, `repository`는 **`sqlmodel`** 사용시 딱히 의미가 없는 모듈이므로 제거하자.\n또한, 지난 글에서 임시로 생성한 `models`의 위치가 `db`의 하위 모듈이 아닌 `app`의 하위 모듈로 결정됐기에 같이 수정한다.\n\n```bash\n❯ rm -rf backend/app/db/repositories\n❯ mv backend/app/db/models backend/app/models\n❯ touch backend/app/models/__init__.py\n❯ mv backend/app/models/base.py backend/app/models/core.py\n❯ mv backend/app/models/temp.py backend/app/models/cleaning.py\n```\n\n```python\n# backend/app/models/core.py\nfrom typing import Any, TypeVar, cast\n\nfrom sqlmodel import Field, SQLModel, Table\n\n_T = TypeVar(\"_T\", bound=SQLModel)\n\n\nclass fix_parse_obj_model(SQLModel):\n    \"\"\"\n    sqlmodel에서 parse_obj 리턴값 정상적으로 수정하기 전까지 사용\n    \"\"\"\n    @classmethod\n    def parse_obj(cls: type[_T], obj: Any, update: dict[str, Any] | None = None) -\u003e _T:\n        return cast(_T, super().parse_obj(obj, update))\n\n\nclass base_model(fix_parse_obj_model):\n    @classmethod\n    def get_table(cls) -\u003e Table:\n        if (table := getattr(cls, \"__table__\", None)) is None:\n            raise ValueError(\"not table\")\n        return table\n\n\nclass id_model(fix_parse_obj_model):\n    id: int | None = Field(None, primary_key=True)\n```\n\n```python\n# backend/app/models/cleaning.py\nfrom enum import Enum\n\nfrom pydantic import condecimal\nfrom sqlmodel import Field\n\nfrom .core import base_model, id_model\n\nprice_decimal_type = condecimal(max_digits=10, decimal_places=2)\n\n\nclass cleaning_type_enum(str, Enum):\n    dust_up = \"dust_up\"\n    spot_clean = \"spot_clean\"\n    full_clean = \"full_clean\"\n\n\nclass cleaning_base(base_model):\n    name: str | None = None\n    description: str | None = None\n    cleaning_type: cleaning_type_enum = cleaning_type_enum.spot_clean\n    price: price_decimal_type | None = None\n\n\nclass cleaning_create(cleaning_base):\n    name: str\n    price: price_decimal_type\n\n\nclass cleaning_update(cleaning_base):\n    cleaning_type: cleaning_type_enum | None = None\n\n\nclass cleanings(id_model, cleaning_base, table=True):\n    name: str = Field(index=True)\n    cleaning_type: cleaning_type_enum = Field(\n        cleaning_type_enum.spot_clean,\n        sa_column_kwargs={\"server_default\": cleaning_type_enum.spot_clean},\n    )\n    price: price_decimal_type\n\n\nclass cleaning_public(id_model, cleaning_base):\n    ...\n```\n\n```python\n# backend/app/db/migrations/versions/f721febf752b_create_account_table.py\n(...)\n\ndef create_cleanings_table() -\u003e None:\n    import sys\n    from pathlib import Path\n    sys.path.append(Path(__file__).resolve().parents[4].as_posix())\n    from app.models.cleaning import cleanings\n\n(...)\n```\n\n`get_table` 메소드가 정의된 `base_model`을 상속하는 5개 모델은 각 리소스에 사용될 패턴을 보여준다.\n\n\u003e - `cleaning_base`: 공유 속성\n\u003e - `cleaning_create`: 새로운 리소스를 생성 ~ **POST**\n\u003e - `cleaning_update`: 기존 리소스를 수정 ~ **PUT**\n\u003e - `cleaning`: 데이터베이스에 정의될 테이블이자 레코드 ~ **GET**, **POST**, **PUT**,...\n\u003e - `cleaning_public`: 레코드에 대한 반환 형태 ~ **GET**, **POST**, **PUT**,...\n\n### `session` 모듈 정의\n\n원 작성자인 **jeffastor**는 이후 `repository` 모듈을 생성하여 CRUD 과정에 필요한 프로세스를 구현했지만, **`sqlmodel`** 을 사용하기에 그러한 과정이 따로 필요하지 않다.\n앱에서 사용할 `session`에 대해서만 따로 정의한다.\n\n```bash\n❯ touch backend/app/db/session.py\n```\n\n```python\n# backend/app/db/session.py\nfrom typing import AsyncIterator\n\nfrom fastapi import Depends, Request\nfrom sqlalchemy.ext.asyncio.engine import AsyncEngine\nfrom sqlmodel.ext.asyncio.session import AsyncSession\n\n\nasync def get_database(request: Request) -\u003e AsyncEngine:\n    if (engine := getattr(request.app.state, \"_db\", None)) is None:\n        raise AttributeError(\"there is no database engine in request as state\")\n    return engine\n\n\nasync def get_session(\n    engine: AsyncEngine = Depends(get_database),\n) -\u003e AsyncIterator[AsyncSession]:\n    async with AsyncSession(engine, autoflush=False, autocommit=False) as session:\n        yield session\n```\n\n## `cleanings` 모델 `api` 예시\n\n### `cleanings` 레코드 추가 `POST` `api` 예시\n\n이제 세션을 활용한 간단한 형태의 **POST** api를 생성한다.\n\n```python\n# backend/app/api/routes/cleanings.py\nfrom fastapi import APIRouter, Body, Depends\nfrom sqlmodel.ext.asyncio.session import AsyncSession\nfrom starlette.status import HTTP_201_CREATED\n\nfrom ...db.session import get_session\nfrom ...models.cleaning import cleaning_create, cleaning_public, cleanings\n\nrouter = APIRouter()\n\n\n@router.get(\"/\")\nasync def get_all_cleanings() -\u003e list[dict]:\n    cleanings = [\n        {\n            \"id\": 1,\n            \"name\": \"My house\",\n            \"cleaning_type\": \"full_clean\",\n            \"price_per_hour\": 29.99,\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Someone else's house\",\n            \"cleaning_type\": \"spot_clean\",\n            \"price_per_hour\": 19.99,\n        },\n    ]\n    return cleanings\n\n\n@router.post(\n    \"/\",\n    response_model=cleaning_public,\n    name=\"cleanings:create-cleaning\",\n    status_code=HTTP_201_CREATED,\n)\nasync def create_new_cleaning(\n    new_cleaning: cleaning_create = Body(..., embed=True),\n    session: AsyncSession = Depends(get_session),\n) -\u003e cleanings:\n    # data = cleanings.from_orm(new_cleaning) 으로 해도 가능\n    # exclude_none=True, exclude_unset=True 옵션을 위해 parse_obj 사용\n    data = cleanings.parse_obj(\n        new_cleaning.dict(\n            exclude_none=True,\n            exclude_unset=True,\n        )\n    )\n    session.add(data)\n    await session.flush()\n    await session.commit()\n    await session.refresh(data)\n\n    return data\n```\n\n이제 **`docker`** 로 서비스를 실행하고, [http://localhost:8000/docs](http://localhost:8000/docs)에서 생성한 **POST** api가 정상적으로 작동하는지 확인한다.\n\n`body`에 값을\n\n```yaml\n{\n  'new_cleaning':\n    { 'name': 'string', 'description': 'string', 'cleaning_type': 'asd', 'price': 0 },\n}\n```\n\n이렇게 주면\n\n```yaml\n{\n  'detail':\n    [\n      {\n        'loc': ['body', 'new_cleaning', 'cleaning_type'],\n        'msg': \"value is not a valid enumeration member; permitted: 'dust_up', 'spot_clean', 'full_clean'\",\n        'type': 'type_error.enum',\n        'ctx': { 'enum_values': ['dust_up', 'spot_clean', 'full_clean'] },\n      },\n    ],\n}\n```\n\n이렇게 왜 에러(422)가 나는지 친절하게 설명도 해준다.\n정상적인 값을 넣으면\n\n```yaml\n{\n  'new_cleaning':\n    { 'name': 'test', 'description': 'test', 'cleaning_type': 'dust_up', 'price': 123 },\n}\n```\n\n설정한대로\n\n```yaml\n{\n  'name': 'test',\n  'description': 'test',\n  'cleaning_type': 'dust_up',\n  'price': 123,\n  'id': 1,\n}\n```\n\n`cleaning_public`의 스키마에 맞게 값을 반환(201)한다. 만약 `cleaning_public`가 `id_model`을 상속하도록 정의하지 않았다면 `id`속성은 생략된 채로 반환됐을 것이다.\n\n\u003e **`fastapi`** 가 위 **POST** api에서 실행한 과정\n\u003e\n\u003e 1. **`json`** 형태의 `body`를 읽는다.\n\u003e 2. `body`의 값을 검증한다. ~ **`pydantic`**\n\u003e 3. 검증 결과에 따라 에러를 반환하거나, 생성한 모델 객체로 계산한 결과를 반환한다.\n\n### `FastAPI`의 `DI` 사용법\n\n`Depends`로 변수를 처리하는 방식이 생소할 수 있다. 사용자의 요청을 처리하는 방식을 사전에 호출 가능한 형태로 정의하고, 그 과정을 한번에 실행한 결과를 파라미터로 받아서 사용할 수 있게 한다.\n\n직전에 생성한 `session.py`에서 `get_database`가 `request`를 받고,\n`get_session`이 `get_database`를 받고,\n`create_new_cleaning`이 `get_session`을 받아서 `session`객체를 파라미터로 사용할 수 있게 한다.\n\n마지막으로, sql 서버에서 정상적으로 레코드가 입력됐는지 확인해본다.\n\n```bash\nbash-5.1# psql -h localhost -U postgres --dbname=postgres\npsql (14.2)\nType \"help\" for help.\n\npostgres=# select * from cleanings;\n name | description | cleaning_type | price  | id\n------+-------------+---------------+--------+----\n test | test        | dust_up       | 123.00 |  1\n(1 row)\n```\n\n다음에는 **`pytest`** 를 활용한다.\n**`alembic`** 처럼 사용해본적이 없기에 기대가 된다.\n","mtime":"2022-08-13T00:12:20.000+09:00","href":"velog/fastapi 튜토리얼 -3- sql 모델 정의","data":{"title":"fastapi 튜토리얼 -3- sql 모델 정의","date":"2022-04-28T21:51:36.869+09:00","tags":["fastapi","python","sqlmodel","@all"],"page":"fastapi 튜토리얼","summary":"fastapi 사용법을 다시 공부할겸, 참고할만한 좋은 예제가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다."}},{"name":"fastapi 튜토리얼 -2- sql 서버 연결","content":"\n**`fastapi`** 사용법을 다시 공부할겸, 참고할만한 좋은 [예제](https://www.jeffastor.com/blog/populating-cleaning-jobs-with-user-offers-in-fastapi)가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다.\n지난번처럼 어쩌다 그만둘 수도 있긴 하지만...\n\n---\n\n## `sql` 서버 연결을 위한 초기 설정\n\n### `docker-compose` 설정\n\nsql 서버는 **`postgres`** 를 사용하기로 한다. 따라서 필요한 파이썬 패키지를 추가로 설치하고, `docker-compose.yml` 파일을 수정한다.\n\n```bash\n❯ poetry add asyncpg sqlalchemy sqlmodel\n❯ poetry export -f requirements.txt --output backend/requirements.txt --without-hashes\n```\n\n```yaml\n# docker-compose.yml\n# prettier-ignore\nversion: \"3.8\"\nservices:\n  server:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile\n    volumes:\n      - ./backend/:/backend/\n    command: uvicorn app.api.server:app --reload --workers 2 --host 0.0.0.0 --port 8000\n    env_file:\n      - ./backend/.env\n    ports:\n      - 8000:8000\n    depends_on:\n      - db\n\n  db:\n    image: postgres:14-alpine\n    volumes:\n      - ./postgres_data:/var/lib/postgresql/data/\n    env_file:\n      - ./backend/.env\n    ports:\n      - 5432:5432\n```\n\n### `git` 구성\n\n이유는 모르겠지만, **jeffastor**는 이전 글부터가 아닌, 이번 글 부터 **`git`** 으로 관리를 시작한다..\n\n```bash\n❯ touch .gitignore\n```\n\n```yaml\n# .gitignore\n# Byte-compiled files\n__pycache__/\n# Environment files\n.env\n```\n\n```bash\n❯ git init\n❯ git add .\n❯ git commit -m \"Dockerized FastAPI app with postgres.\"\n```\n\n### 환경변수 설정\n\n**`postgres`** 및 서버 전반적으로 사용할 환경변수를 설정한다.\n\n```python\n# backend/.env\nSECRET_KEY=supersecret\n\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=postgres\nPOSTGRES_SERVER=db\nPOSTGRES_PORT=5432\nPOSTGRES_DB=postgres\n```\n\n`SECRET_KEY`의 경우, 지금은 기본값으로 사용하지만 나중에 수정할거니 걱정하지 않아도 된다고 한다..\n\n### `config.py` 설정\n\n이제 서버에서 사용할 설정 파일을 생성한다.\n\n```bash\n❯ touch backend/app/core/config.py\n```\n\n```python\n# backend/app/core/config.py\nfrom sqlalchemy.engine.url import URL\nfrom starlette.config import Config\nfrom starlette.datastructures import Secret\n\nconfig = Config(\".env\")\n\nPROJECT_NAME = \"jeffastor_tutor\"\nVERSION = \"1.0.0\"\nAPI_PREFIX = \"/api\"\n\nSECRET_KEY = config(\"SECRET_KEY\", cast=Secret, default=\"CHANGEME\")\n\nPOSTGRES_USER = config(\"POSTGRES_USER\", cast=str)\nPOSTGRES_PASSWORD = config(\"POSTGRES_PASSWORD\", cast=Secret)\nPOSTGRES_SERVER = config(\"POSTGRES_SERVER\", cast=str, default=\"db\")\nPOSTGRES_PORT = config(\"POSTGRES_PORT\", cast=int, default=5432)\nPOSTGRES_DB = config(\"POSTGRES_DB\", cast=str)\n\nDATABASE_URL = config(\n    \"DATABASE_URL\",\n    cast=str,\n    default=URL.create(\n        drivername=\"postgresql+asyncpg\",\n        username=POSTGRES_USER,\n        password=POSTGRES_PASSWORD,\n        host=POSTGRES_SERVER,\n        port=POSTGRES_PORT,\n        database=POSTGRES_DB,\n    ).render_as_string(hide_password=False),\n)\n```\n\n원 예제와 다르게, **`sqlmodel`** 을 사용할 예정이라 url을 다르게 설정했다.\n\n\u003e 설명에 따르면, 기본값이 없는 `config` 객체에 설정된 모든 값은 `.env`파일에서 값을 제공해야하고, 그렇지 않으면 에러가 발생한다고 한다.\n\n## `sql` 서버 연결 스크립트 작성\n\n이제 sql 서버와 연결하기 위한 모듈과 앱 시작/종료 이벤트와 관련한 작업 파일을 생성한다.\n\n```bash\n❯ mkdir backend/app/db\n❯ touch backend/app/db/__init__.py backend/app/db/tasks.py backend/app/db/engine.py  backend/app/core/tasks.py\n```\n\n### 엔진 설정\n\n```python\n# backend/app/db/engine.py\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.pool import QueuePool\n\nfrom ..core.config import DATABASE_URL\n\nengine = create_async_engine(\n    DATABASE_URL, pool_size=10, poolclass=QueuePool, pool_pre_ping=True\n)\n```\n\n```python\n# backend/app/db/tasks.py\nimport logging\nfrom typing import cast\n\nfrom fastapi import FastAPI\nfrom sqlalchemy.ext.asyncio.engine import AsyncEngine\n\nfrom .engine import engine\n\nlogger = logging.getLogger(__name__)\n\n\nasync def connect_to_db(app: FastAPI) -\u003e None:\n    try:\n        async with engine.connect():\n            logger.info(\n                f\"connected db: {engine.url.render_as_string(hide_password=True)}\"\n            )\n        app.state._db = engine\n    except Exception as e:\n        logger.warning(\"--- DB CONNECTION ERROR ---\")\n        logger.warning(e)\n        logger.warning(\"--- DB CONNECTION ERROR ---\")\n\n\nasync def close_db_connection(app: FastAPI) -\u003e None:\n    engine = cast(AsyncEngine, app.state._db)\n    try:\n        await engine.dispose()\n    except Exception as e:\n        logger.warning(\"--- DB DISCONNECT ERROR ---\")\n        logger.warning(e)\n        logger.warning(\"--- DB DISCONNECT ERROR ---\")\n```\n\n```python\n# backend/app/core/tasks.py\nfrom typing import Any, Callable, Coroutine\n\nfrom fastapi import FastAPI\n\nfrom ..db.tasks import close_db_connection, connect_to_db\n\n\ndef create_start_app_handler(app: FastAPI) -\u003e Callable[[], Coroutine[Any, Any, None]]:\n    async def start_app() -\u003e None:\n        await connect_to_db(app)\n\n    return start_app\n\n\ndef create_stop_app_handler(app: FastAPI) -\u003e Callable[[], Coroutine[Any, Any, None]]:\n    async def stop_app() -\u003e None:\n        await close_db_connection(app)\n\n    return stop_app\n```\n\n`AsyncEngine` 인스턴스를 생성하고, 이 인스턴스를 이용해서 앱을 시작할 때와 종료할 때 실행할 두가지 핸들러를 정의했다.\n\n\u003e 기존 글에서는 엔진을 직접 만들기 보다 **`databases`** 를 이용하는데, 일단 이전에 사용한 적 있는 **`sqlmodel`** 로 진행한다.\n\n이 핸들러는 sql 서버 연결이 정상적으로 이루어졌다면, 앱의 `state`에 `_db`라는 속성으로 `AsyncEngine` 인스턴스를 호출할 수 있게 한다.\n그리고 종료할 때, 이 인스턴스와 연결된 모든 세션을 종료한다.\n이제 이 핸들러를 적용한다.\n\n```python\n# backend/app/api/routes/server.py\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import ORJSONResponse\n\nfrom ..core import config, tasks\nfrom .routes import router as api_router\n\n\ndef get_application() -\u003e FastAPI:\n    app = FastAPI(\n        title=config.PROJECT_NAME,\n        version=config.VERSION,\n        default_response_class=ORJSONResponse,\n    )\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    app.add_event_handler(\"startup\", tasks.create_start_app_handler(app))\n    app.add_event_handler(\"shutdown\", tasks.create_stop_app_handler(app))\n\n    app.include_router(api_router, prefix=config.API_PREFIX)\n\n    return app\n\n\napp = get_application()\n```\n\n만약 **`docker`** 실행 시 정상적으로 작동한다면, sql 서버를 사용할 준비가 끝났다.\n\n### `alembic`을 사용한 마이그레이션\n\n이제 **`alembic`** 을 사용한 마이그레이션을 구성한다고 하는데.. 사용해본적이 한번도 없어서 생소한 라이브러리다. 그러니 일단 그대로 따라하는데 중점을 둔다.\n\n```bash\n❯ mkdir backend/app/db/migrations backend/app/db/repositories\n❯ touch backend/app/db/migrations/script.py.mako backend/app/db/migrations/env.py backend/app/db/repositories/__init__.py backend/app/db/repositories/base.py backend/alembic.ini\n```\n\n\u003e **`mako`** 확장자는 [Mako](https://www.makotemplates.org/) 템플릿이라고 한다.\n\n```yaml\n# backend/alembic.ini\n# A generic, single database configuration.\n\n[alembic]\n# path to migration scripts\nscript_location = ./app/db/migrations\n\n# template used to generate migration files\n# file_template = %%(rev)s_%%(slug)s\n\n# sys.path path, will be prepended to sys.path if present.\n# defaults to the current working directory.\n# prepend_sys_path = .\n\n# timezone to use when rendering the date within the migration file\n# as well as the filename.\n# If specified, requires the python-dateutil library that can be\n# installed by adding `alembic[tz]` to the pip requirements\n# string value is passed to dateutil.tz.gettz()\n# leave blank for localtime\n# timezone =\n\n# max length of characters to apply to the\n# \"slug\" field\n# truncate_slug_length = 40\n\n# set to 'true' to run the environment during\n# the 'revision' command, regardless of autogenerate\n# revision_environment = false\n\n# set to 'true' to allow .pyc and .pyo files without\n# a source .py file to be detected as revisions in the\n# versions/ directory\n# sourceless = false\n\n# version location specification; This defaults\n# to test/versions.  When using multiple version\n# directories, initial revisions must be specified with --version-path.\n# The path separator used here should be the separator specified by \"version_path_separator\" below.\n# version_locations = %(here)s/bar:%(here)s/bat:test/versions\nversion_locations = ./app/db/migrations/versions\n\n# version path separator; As mentioned above, this is the character used to split\n# version_locations. The default within new alembic.ini files is \"os\", which uses os.pathsep.\n# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.\n# Valid values for version_path_separator are:\n#\n# version_path_separator = :\n# version_path_separator = ;\n# version_path_separator = space\n# version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.\n\n# the output encoding used when revision files\n# are written from script.py.mako\n# output_encoding = utf-8\n\n# sqlalchemy.url = driver://user:pass@localhost/dbname\n\n\n[post_write_hooks]\n# post_write_hooks defines scripts or Python functions that are run\n# on newly generated revision scripts.  See the documentation for further\n# detail and examples\n\n# format using \"black\" - use the bash_scripts runner, against the \"black\" entrypoint\n# hooks = black\n# black.type = bash_scripts\n# black.entrypoint = black\n# black.options = -l 79 REVISION_SCRIPT_FILENAME\n\n# Logging configuration\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = bash\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARN\nhandlers = bash\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARN\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_bash]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n```\n\n```python\n# backend/app/db/migrations/script.py.mako\n\"\"\"${message}\n\nRevision ID: ${up_revision}\nRevises: ${down_revision | comma,n}\nCreate Date: ${create_date}\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n${imports if imports else \"\"}\n\n# revision identifiers, used by Alembic.\nrevision = ${repr(up_revision)}\ndown_revision = ${repr(down_revision)}\nbranch_labels = ${repr(branch_labels)}\ndepends_on = ${repr(depends_on)}\n\n\ndef upgrade():\n    ${upgrades if upgrades else \"pass\"}\n\n\ndef downgrade():\n    ${downgrades if downgrades else \"pass\"}\n```\n\n각종 설정이 추가되는데, **`mako`** 템플릿 파일은 마이그레이션 스크립트를 생성하고, 그 과정을 로그로 남기는 것이라고 한다... 실제로 봐야 알 수 있을 듯.\n\n이제 끝으로 `env.py`를 작성해야하는데, 이제보니 첫 과정에서 **`alembic`** 를 추가하지 않았기에, 그 과정을 함께한다.\n\n```bash\n❯ poetry add alembic\n❯ poetry export -f requirements.txt --output backend/requirements.txt --without-hashes\n```\n\n```python\n# backend/app/db/migrations/env.py\nimport asyncio\nimport logging\nimport pathlib\nimport sys\nfrom logging.config import fileConfig\nfrom typing import cast\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom sqlalchemy.ext.asyncio import AsyncEngine\nfrom sqlalchemy.future.engine import Engine\n\nsys.path.append(str(pathlib.Path(__file__).resolve().parents[3]))\nfrom app.core.config import DATABASE_URL  # noqa\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\nlogger = logging.getLogger(\"alembic.env\")\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = None\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline():\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    # url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        # url=url,\n        url=DATABASE_URL,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection):\n    context.configure(connection=connection, target_metadata=target_metadata)\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\nasync def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    config.set_main_option(\"sqlalchemy.url\", DATABASE_URL)\n    connectable = AsyncEngine(\n        cast(\n            Engine,\n            engine_from_config(\n                config.get_section(config.config_ini_section),\n                prefix=\"sqlalchemy.\",\n                poolclass=pool.NullPool,\n                future=True,\n            ),\n        )\n    )\n\n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n\n    await connectable.dispose()\n\n\nif context.is_offline_mode():\n    logger.info(\"Running migrations offline\")\n    run_migrations_offline()\nelse:\n    logger.info(\"Running migrations online\")\n    asyncio.run(run_migrations_online())\n```\n\n이제 첫번째 마이그레이션을 **`docker`** 내부에서 실행하면 된다고 하는데...\n\n```bash\nroot@bad23fe368a6:/backend# ls\nDockerfile  alembic.ini  app  requirements.txt  tests\nroot@bad23fe368a6:/backend# alembic revision -m \"create account table\"\n  Generating\n  /backend/app/db/migrations/versions/f721febf752b_create_account_table.py\n  ...  done\n```\n\n마이그레이션이 정상적으로 진행됐다!\n그리고 다음과 같은 파일을 확인할 수 있다.\n\n```python\n# backend/app/db/migrations/versions/f721febf752b_create_account_table.py\n\"\"\"create account table\n\nRevision ID: f721febf752b\nRevises:\nCreate Date: 2022-04-27 17:21:25.945460\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'f721febf752b'\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    pass\n\n\ndef downgrade():\n    pass\n```\n\n### 마이그레이션 테스트 모델 생성 및 확인\n\n`env.py` 파일의\n\n```python\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = None\n```\n\n주석에 적혀있듯이, `autogenerate` 옵션을 사용하면 자동으로 모델 기반으로 생성해준다고 한다. 다만 이 기능이 완벽하지는 않은지, 원 작성자 **jeffastor**는 마이그레이션으로 작성된 스크립트에\n\n```python\ndef create_cleanings_table() -\u003e None:\n    op.create_table(\n        \"cleanings\",\n        sa.Column(\"id\", sa.Integer, primary_key=True),\n        sa.Column(\"name\", sa.Text, nullable=False, index=True),\n        sa.Column(\"description\", sa.Text, nullable=True),\n        sa.Column(\"cleaning_type\", sa.Text, nullable=False, server_default=\"spot_clean\"),\n        sa.Column(\"price\", sa.Numeric(10, 2), nullable=False),\n    )\n```\n\n이라는 함수를 새로 작성해서 진행했다. **`sqlmodel`** 을 사용하는 만큼, 임시로 모델을 생성하고, 그 모델에서 자동으로 생성된 테이블에서 `Column`을 추출하는 방식으로 진행한다. 추후 모델을 정의할 파일 위치를 확인하면 옮기고 수정할 예정.\n\n```bash\n❯ mkdir backend/app/db/models\n❯ touch backend/app/db/models/base.py backend/app/db/models/temp.py\n```\n\n```python\n# touch backend/app/db/models/base.py\nclass base_model(SQLModel):\n    @classmethod\n    def get_table(cls) -\u003e Table:\n        if (table := getattr(cls, \"__table__\", None)) is None:\n            raise ValueError(\"not table\")\n        return table\n```\n\n```python\n# backend/app/db/models/temp.py\nfrom pydantic import condecimal\nfrom sqlmodel import Field\n\nfrom .base import base_model\n\n\nclass cleanings(base_model, table=True):\n    id: int | None = Field(None, primary_key=True)\n    name: str = Field(index=True)\n    description: str | None = None\n    cleaning_type: str = Field(\n        (_default_cleaning_type := \"spot_clean\"),\n        sa_column_kwargs={\"server_default\": _default_cleaning_type},\n    )\n    price: condecimal(max_digits=10, decimal_places=2)  # type: ignore\n```\n\n이 튜토리얼은 청소 관련 주제로 작성되기에, 테이블 이름이 `cleanings`이다.\n\n\u003e - `id`: 각 항목에 대한 고유한 식별값.\n\u003e - `name`: 해당 항목에 대한 이름. `index=True` 옵션으로 인해 더 빠른 조회가 가능하다.\n\u003e - `description`: 해당 항목에 대한 설명이지만, `null`값(파이썬에서는 `None`값)이 가능하다.\n\u003e - `cleaning_type`: 해당 항목의 타입\n\u003e - `price`: 해당 항목의 가격\n\n모델을 정의했으니, 이제 예제를 따라 함수를 정의한다.\n\n```python\n# backend/app/db/migrations/versions/f721febf752b_create_account_table.py\n(...)\n\ndef create_cleanings_table() -\u003e None:\n    import sys\n    from pathlib import Path\n    sys.path.append(Path(__file__).resolve().parents[4].as_posix())\n    from app.db.models.temp import cleanings\n\n    table = cleanings.get_table()\n\n    op.create_table(\n        table.name,\n        *table.columns\n    )\n\ndef upgrade():\n    create_cleanings_table()\n\n\ndef downgrade():\n    op.drop_table('cleanings')\n```\n\n대부분의 경우 **`sqlmodel`** 의 `Field`의 변수로 가능하고, `server_default`에 대해서만 따로 `sa_column_kwargs`로 처리했다. 이렇게 하지 않아도 쿼리에 정상적으로 기본값이 적용되는 것으로 알고 있지만, 혹시 몰라서..\n\n이제 마이그레이션을 진행한다.\n\n```bash\nroot@9c425f594efa:/backend# alembic upgrade head\nINFO  [alembic.env] Running migrations online\nINFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\nINFO  [alembic.runtime.migration] Running upgrade  -\u003e f721febf752b, create account table\n```\n\n테이블이 정상적으로 생성됐는지, `db` 컨테이너에서 **`psql`** 을 이용해 확인해본다.\n\n```bash\nbash-5.1# psql -h localhost -U postgres --dbname=postgres\npsql (14.2)\nType \"help\" for help.\n\npostgres=# select * from cleanings;\n id | name | description | cleaning_type | price\n----+------+-------------+---------------+-------\n(0 rows)\n```\n\n정상적으로 생성 된 것을 확인했다.\n","mtime":"2022-08-13T00:12:20.000+09:00","href":"velog/fastapi 튜토리얼 -2- sql 서버 연결","data":{"title":"fastapi 튜토리얼 -2- sql 서버 연결","date":"2022-04-28T19:51:57.390+09:00","tags":["fastapi","alembic","postgres","python","@all"],"page":"fastapi 튜토리얼","summary":"fastapi 사용법을 다시 공부할겸, 참고할만한 좋은 예제가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다."}},{"name":"fastapi 튜토리얼 -1- 초기 설정","content":"\n**`fastapi`** 사용법을 다시 공부할겸, 참고할만한 좋은 [예제](https://www.jeffastor.com/blog/populating-cleaning-jobs-with-user-offers-in-fastapi)가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다.\n지난번처럼 어쩌다 그만둘 수도 있긴 하지만...\n\n---\n\n## 초기 설정\n\n### 파이썬 설정\n\n우선 다음 명령어로 가상 파이썬 환경부터 잡아줬다.\n\n```bash\n❯ pyenv virtualenv 3.10.4 jeffastor_tutor\n❯ pyenv local jeffastor_tutor\n```\n\n실제로는 **`docker`** 로 실행하겠지만, **`vscode`** 를 사용하면서 **`pylance`** 의 자동완성과 타입 추론 기능을 사용하기 위해, 따로 만들었다.\n\n그리고 필수 패키지와 개인적으로 선호하는 **`orjson`** 을 설치한다.\n\n```bash\n❯ poetry init\n❯ poetry add fastapi \"uvicorn[standard]\" orjson\n❯ poetry add --dev black isort\n❯ cat pyproject.toml\n[tool.poetry]\nname = \"jeffastor_tutor\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"phi \u003cphi.friday@gmail.com\u003e\"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nfastapi = \"^0.75.2\"\nuvicorn = {extras = [\"standard\"], version = \"^0.17.6\"}\norjson = \"^3.6.8\"\n\n[tool.poetry.dev-dependencies]\nblack = \"^22.3.0\"\nisort = \"^5.10.1\"\n\n[build-system]\nrequires = [\"poetry-core\u003e=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n```\n\n\u003e - fastapi - 백엔드 구축에 사용할 프레임워크\n\u003e - uvicorn - fastapi 앱을 사용하기 위한 asgi 서버\n\u003e - orjson - 좀 더 빠르고, 정확하고, 다양하게 파이썬 객체를 json으로 변환해주는 라이브러리\n\n### `fastapi` 기본 구성 설정\n\n**JeffAstor**가 제시한 디렉토리/모듈 구성은 대부분 그대로 가져갈 생각이다.\n\n```bash\n❯ mkdir backend backend/app backend/tests\n❯ mkdir backend/app/api backend/app/core\n❯ touch backend/app/api/__init__.py backend/app/api/server.py\n```\n\n```python\n# backend/app/api/server.py\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import ORJSONResponse\n\n\ndef get_application() -\u003e FastAPI:\n    app = FastAPI(title='jeffastor_tutor', default_response_class=ORJSONResponse)\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    return app\n\napp = get_application()\n```\n\n`CORSMiddleware`에 대해서는 지금 신경쓸 필요 없으며, 나중에 관련해서 자세히 설명하겠다고 한다.\n\n`default_response_class`는 **`fastapi`** 의 `response`를 직렬화 할 때 **`orjson`** 을 사용하기 위해 `ORJSONResponse`를 지정했다.\n\n### `docker` 설정\n\n이제 서버를 실행할 **`docker`** 를 설정한다.\n\n```bash\n❯ poetry export -f requirements.txt --output backend/requirements.txt --without-hashes\n❯ touch docker-compose.yml backend/Dockerfile backend/.env\n```\n\n```Dockerfile\n# backend/Dockerfile\nFROM python:3.10-slim-bullseye\nWORKDIR /backend\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONBUFFERED 1\n# install system dependencies\n# RUN apt-get update \\\n#   \u0026\u0026 apt-get -y install netcat gcc postgresql \\\n#   \u0026\u0026 apt-get clean\n# install python dependencies\n# RUN pip install --upgrade pip\nCOPY ./requirements.txt /backend/requirements.txt\nRUN pip install -r requirements.txt\nCOPY . /backend\n```\n\n```yaml\n# docker-compose.yml\n# prettier-ignore\nversion: \"3.8\"\nservices:\n  server:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile\n    volumes:\n      - ./backend/:/backend/\n    command: uvicorn app.api.server:app --reload --workers 2 --host 0.0.0.0 --port 8000\n    env_file:\n      - ./backend/.env\n    ports:\n      - 8000:8000\n```\n\nsql 서버는 다른 컨테이너로 관리할 예정이기에 주석처리했다.\n\n이제 **`docker`** 를 실행해보자.\n\n```bash\n❯ docker-compose up --build\n\n```\n\n브라우저에서 `localhost:8000`에 접속해보면\n\n```yaml\n{ 'detail': 'Not Found' }\n```\n\n아직 아무런 라우터를 추가하지 않았기에 나오는 기본값이 출력된다.\n\n## 기본 라우터 생성\n\n서버를 중단하지 않고, 이어서 라우터를 추가한다.\n\n```bash\n❯ mkdir backend/app/api/routes\n❯ touch backend/app/api/routes/__init__.py\n❯ touch backend/app/api/routes/cleanings.py\n```\n\n```python\n# backend/app/api/routes/cleanings.py\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\n\n@router.get(\"/\")\nasync def get_all_cleanings() -\u003e list[dict]:\n    cleanings = [\n        {\"id\": 1, \"name\": \"My house\", \"cleaning_type\": \"full_clean\", \"price_per_hour\": 29.99},\n        {\"id\": 2, \"name\": \"Someone else's house\", \"cleaning_type\": \"spot_clean\", \"price_per_hour\": 19.99}\n    ]\n    return cleanings\n```\n\n```python\n# backend/app/api/routes/__init__.py\nfrom fastapi import APIRouter\n\nfrom .cleanings import router as cleanings_router\n\nrouter = APIRouter()\n\nrouter.include_router(cleanings_router, prefix=\"/cleanings\", tags=[\"cleanings\"])\n```\n\n원문은 절대참조로 작성했지만, **`docker`** 실행 환경에서는 정상적으로 작동하지만, 편집 환경에서는 **`pylance`** 가 정상적으로 인식하지 못하므로 상대참조로 변경했다.\n\n끝으로 `get_application`함수를 수정한다.\n\n```python\n# backend/app/api/server.py\n(...)\nfrom .routes import router as api_router\n\n\ndef get_application() -\u003e FastAPI:\n    app = FastAPI(title='jeffastor_tutor', default_response_class=ORJSONResponse)\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    app.include_router(api_router, prefix='/api')\n\n    return app\n(...)\n```\n\n**`uvicorn`** 의 `reload` 옵션을 켰기 때문에, 별다른 조작 없이 수정사항이 반영된 상태로 서버가 다시 실행된다. [http://localhost:8000/api/cleanings/](http://localhost:8000/api/cleanings/)에서 다음과 같은 결과를 확인할 수 있다.\n\n```yaml\n[\n  {\n    'id': 1,\n    'name': 'My house',\n    'cleaning_type': 'full_clean',\n    'price_per_hour': 29.99,\n  },\n  {\n    'id': 2,\n    'name': \"Someone else's house\",\n    'cleaning_type': 'spot_clean',\n    'price_per_hour': 19.99,\n  },\n]\n```\n\n다음 챕터에서는 sql 서버를 연결하고, **`pytest`** 를 이용하여 테스트를 진행한다.\n\n참고로, **`docker`** 로 실행된 서버는\n\n```bash\n❯ docker-compose down\n```\n\n으로 종료가 가능하다.\n","mtime":"2022-08-13T00:12:20.000+09:00","href":"velog/fastapi 튜토리얼 -1- 초기 설정","data":{"title":"fastapi 튜토리얼 -1- 초기 설정","date":"2022-04-27T23:30:14.239+09:00","tags":["fastapi","python","@all"],"page":"fastapi 튜토리얼","summary":"fastapi 사용법을 다시 공부할겸, 참고할만한 좋은 예제가 있어서 이 시리즈를 약간의 변경을 주고 따라가보려 한다."}},{"name":"파이썬 코드를 깔끔하게, 그리고 일관성 있게 작성하는법","content":"스크립트를 작성하다보면 일관성있게 작성하기가 쉽지 않다.\n```python\ndata = [\n    (x, y)\n    for x, y in zip(range(5), range(20, 24))\n    if (x % 3 and y % 2) or (x % 5 and y % 7)\n]\n```\n이렇게 작성하든\n```python\ndata = [(x,y) for x,y in zip(range(5),range(20,24)) if (x%3 and y%2) or (x%5 and y%7)]\n```\n저렇게 작성하든 결국 결과는 같기 때문이다.\n\n이는 특히 협업시 두드러지게 나타나고, 파이썬의 장점인 **실행할 수 있는 의사코드**라는 점이 많이 퇴색된다.\n\n이러한 경우를 해결하기 위해, 그렇다면 **정해진 하나의 규칙으로, 전부 밀어버리면 되지 않냐**는 생각으로 만들어진 패키지가 있다. 바로 `black`이다.\n\n## 코드를 일관성있게 만드는 black\n***\n설치도 사용도 간단하다. `pip install black`으로 설치가 가능하고, `python -m black {source_file_or_directory}`으로 실행이 가능하다.\n\n**그런데 이것도 사실 꽤 귀찮다.** 스크립트를 수정하고 `black`을 계속 돌려줘야 하는데, 이걸 어떻게 다 기억하고 있을까.\n\n그래서 많이 쓰이는 `vscode`에서는, 파이썬 스크립트를 저장할 때 자동으로 `black`이 적용되도록 하는 옵션이 따로 있다. 다음 값을 `settings.json`에 추가하면 된다.\n```json\n{\n    \"python.formatting.provider\": \"black\",\n    \"editor.formatOnSave\": true\n}\n```\n\n이제 `vscode`에서 다음과 같은 코드를 작성하고\n```python\ndef test1(a,  b):\n    c=a+b\n    return c\n\ndef test2(\n    a,\n    b\n):\n    c = (\n        a*b\n    )\n    return c\n```\n저장을 하면 다음과 같이 변한다.\n```python\ndef test1(a, b):\n    c = a + b\n    return c\n\n\ndef test2(a, b):\n    c = a * b\n    return c\n\n```\n`black`의 변환 규칙은 [여기](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html)에서 확인 할 수 있다.\n\n### 하지만 완벽하지는 않다.\n\n하지만 `black`도 2% 부족한 부분이 있다. 각종 모듈을 import할 때, 해당 구문에 대한 정리가 단순히 88자 글자 제한을 넘기지 않는 정도에서 그치는 것이다.\n\n만약 다음과 같은 import구문이 있다면\n```python\nfrom datetime import datetime\nfrom anyio.abc import ObjectReceiveStream, ObjectSendStream\nfrom random import uniform\nfrom anyio import create_memory_object_stream, create_task_group, Semaphore, CapacityLimiter\n```\n`black`은 이렇게 까지만 수정된다.\n```python\nfrom datetime import datetime\nfrom anyio.abc import ObjectReceiveStream, ObjectSendStream\nfrom random import uniform\nfrom anyio import (\n    create_memory_object_stream,\n    create_task_group,\n    Semaphore,\n    CapacityLimiter,\n)\n```\n내가 import하는 모듈이 기본 라이브러리인지, 내가 따로 설치한 라이브러리인지, 로컬 패키지 인지 정리할 수 있다면 참 좋을텐데... 다행히 **그걸 위한 패키지가 있다.** 바로 `isort`다.\n\n## import 구문도 빠짐없이 정리\n\n`isort`의 readme 예시에 따르면, 다음과 같은 코드를\n```python\nfrom my_lib import Object\n\nimport os\n\nfrom my_lib import Object3\n\nfrom my_lib import Object2\n\nimport sys\n\nfrom third_party import lib15, lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11, lib12, lib13, lib14\n\nimport sys\n\nfrom __future__ import absolute_import\n\nfrom third_party import lib3\n```\n다음과 같이 깔끔하게 정리해준다.\n```python\nfrom __future__ import absolute_import\n\nimport os\nimport sys\n\nfrom third_party import (lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8,\n                         lib9, lib10, lib11, lib12, lib13, lib14, lib15)\n\nfrom my_lib import Object, Object2, Object3\n```\n\n정말 깔끔하게 변했다. 하지만 `from third_party import (...)`에서 확인할 수 있듯이, `black`의 규칙과 맞지 않는 부분이 있다. 다행히 최근 `isort`는 **`black`사용자를 위한 설정값을 따로 만들어놨다.**\n\n나는 `vscode`에서 저장할 때 자동으로 적용되길 바라기 때문에, `settings.json`에 다음과 같은 값을 추가했다.\n```json\n{\n    \"python.sortImports.args\": [\n        \"--profile\",\n        \"black\"\n    ],\n    \"[python]\": {\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": true\n        }\n    }\n}\n```\n\n이 설정값을 저장하고 나면, 이전 코드는 다음과 같이 변한다.\n```python\nfrom datetime import datetime\nfrom random import uniform\n\nfrom anyio import (\n    CapacityLimiter,\n    Semaphore,\n    create_memory_object_stream,\n    create_task_group,\n)\nfrom anyio.abc import ObjectReceiveStream, ObjectSendStream\n```\n\n## 결론\n***\n`vscode`에서 파이썬 코드를 깔끔하게 작성하고 싶다면, `black`과 `isort`를 이용하기 위해, `settings.json`에 다음 값을 추가하면 된다.\n```json\n{\n    \"python.formatting.provider\": \"black\",\n    \"editor.formatOnSave\": true,\n    \"python.sortImports.args\": [\n        \"--profile\",\n        \"black\"\n    ],\n    \"[python]\": {\n        \"editor.codeActionsOnSave\": {\n            \"source.organizeImports\": true\n        }\n    }\n}\n```\n","mtime":"2022-08-13T00:12:20.000+09:00","href":"velog/파이썬 코드를 깔끔하게, 그리고 일관성 있게 작성하는법","data":{"title":"파이썬 코드를 깔끔하게, 그리고 일관성 있게 작성하는법","date":"2021-11-29T23:56:34.516+09:00","tags":["black","isort","python","vscode","@all"],"page":null,"summary":"black과 isort를 vscode에서 사용하자"}},{"name":"파이썬으로 동시성 프로그래밍을 쉽게 하는법 - 2","content":"\n비동기 프로그래밍의 핵심은 무엇일까? 저는 **작업을 생성**하고, 각 작업의 **전환점을 명시**하고, 각 작업간 **정보를 주고받는 것**이라고 생각합니다.\n\n**전환점을 명시**하는 것은 `python`에 특별한 키워드로 추가됐기에, 명확합니다. 각 작업간 전환이 가능하다(`awaitable`이라고 하는 것 같습니다)는 것을 알리는 `async`와, 전환점을 명확하게 알리는 `await`입니다.\n\n그렇다면 **작업을 생성**하고, **정보를 주고받는 것**은 어떻게 해야할까요?\n\n## `anyio`의 작업 생성\n\n`anyio`는 `trio`의 작업 생성 방식을 따라합니다. 그리고 `trio`는 **암시적 동시성**이 없습니다. 따라서 모든 기능은 위에서 아래로 실행합니다.\n\n`trio`는 사용자가 작업을 생성할 때, 그 작업에 대한 책임을 지도록 설계되었습니다. 이 설계는 `async with` 블록으로 나타내며, 이 블록에서 `start_soon`메소드로 호출된 모든 `awaitable` 함수는 동시에 실행되는 하나의 작업으로 생성됩니다. `trio`는 이것을 `nursery`라고 명명했습니다.\n\n`anyio`에서는 이러한 과정을 `task_group`이라 명명해서 보다 직관적으로 알 수 있게 했습니다. 실제 코드로 확인해보겠습니다.\n\n```python\nfrom datetime import datetime\n\nimport anyio\n\n\nasync def just_sleep(num: int, second: float):\n    print(f\"{datetime.today()}:: {num=}, {second=} sleep start\")\n    await anyio.sleep(second)\n    print(f\"{datetime.today()}:: {num=}, {second=} sleep end\")\n\n\nasync def main():\n    async with anyio.create_task_group() as task_group:\n        for num in range(5):\n            task_group.start_soon(just_sleep, num, 5)\n        return\n\n\nif __name__ == \"__main__\":\n    print(f\"{datetime.today()}:: main start\")\n    result = anyio.run(main)\n    print(f\"{datetime.today()}:: main end??? {result=}\")\n```\n\n이 코드에서 실제로 실행되는 함수는 `main` 함수입니다. 이 함수의 코드를 확인해보면, 우선 `async with anyio.create_task_group`으로 `task_group`을 생성합니다. 그리고 이 `task_group` 블록 안에서 `task_group.start_soon`메소드를 이용하여 5개의 `just_sleep`을 호출했습니다.\n\n따라서 이 `task_group` 블록에는, 총 5개의 `just_sleep` 작업이 예정되어있습니다. 하지만 `task_group` 블록을 나가기 전에, `return` 키워드를 작성해서, `task_group` 블록에 5개의 작업을 호출한 직후 해당 함수를 종료하도록 했습니다.\n\n만약 의도한대로 실행된다면, 5개의 `just_sleep` 작업이 생성되지만, 그 직후 main 함수는 `None`을 반환하고, main end??? 는 main start 출력 이후 1초 내에 출력될 것입니다.\n\n```log\n2021-11-29 17:56:17.798277:: main start\n2021-11-29 17:56:17.800793:: num=0, second=5 sleep start\n2021-11-29 17:56:17.800829:: num=1, second=5 sleep start\n2021-11-29 17:56:17.800853:: num=2, second=5 sleep start\n2021-11-29 17:56:17.800869:: num=3, second=5 sleep start\n2021-11-29 17:56:17.800884:: num=4, second=5 sleep start\n2021-11-29 17:56:22.806281:: num=0, second=5 sleep end\n2021-11-29 17:56:22.806401:: num=1, second=5 sleep end\n2021-11-29 17:56:22.806443:: num=2, second=5 sleep end\n2021-11-29 17:56:22.806479:: num=3, second=5 sleep end\n2021-11-29 17:56:22.806513:: num=4, second=5 sleep end\n2021-11-29 17:56:22.807098:: main end??? result=None\n```\n\n하지만 실제 결과는, `return`과 무관하게 생성한 작업이 모두 끝난 이후 `return`이 실행됩니다. 즉, 사용자는 작업을 생성할 때, 각 작업을 어떤식으로든 완료되는 것을 확인 할 의무가 있습니다. 만약 작업 도중 `return`을 실행할 일이 있다면, 그에 맞는 조건을 설정하여 해당 `task_group`을 종료시킨 다음 `return`하는 것이 맞습니다.\n\n말이 길어졌기에, 요약하자면\n\n\u003e 1. `awaitable` 함수는 `async with anyio.create_task_group` 블록 내부에서 `task_group.start_soon` 메소드로 호출한다.\n\u003e 2. 생성된 작업은, 어떤식으로든 완료시켜야 한다.\n\n이 2가지만 기억해도 큰 문제가 없습니다.\n\n## `anyio`의 정보 주고받기\n\nanyio에서 정보를 주고받을때 사용하는 것을 `stream`이라고 합니다. 이 `stream`은 크게 두가지로 분류되는데, 바이트 스트림과 객체 스트림입니다.\n\n객체 스트림은 기존에 사용하던 큐와 거의 같은 형태로 사용이 가능합니다. `stream`의 버퍼 사이즈를 지정하고, 버퍼 사이즈 만큼 객체를 입력하고, 버퍼에 객체가 있으면 그 객체를 가져오는 간단한 방식입니다. `trio`에서는 `channel`이라고 명명합니다.\n\n바이트 스트림은 약간 다릅니다. 만약 사용자가 `b'qwe'`, `b'rty'`라는 두개의 바이트 객체를 입력했다면, 이 스트림에서 객체를 받을 때, `b'qwe'`, `b'rty'`라고 받을 수도 있지만, `b'q'`, `b'wer'`, `b'ty'`라고 받을 수도 있고, `b'qwert'`, `b'y'`라고 받을 수도 있습니다. `trio`에서는 `stream`이라고 명명합니다.\n\n실제로 `stream`을 사용하는 코드로 확인하겠습니다.\n\n```python\nfrom datetime import datetime\nfrom random import uniform\n\nimport anyio\nfrom anyio.abc import ObjectReceiveStream, ObjectSendStream\n\n\nasync def ping(num: int, send: ObjectSendStream):\n    print(f\"{datetime.today()}:: {num=}, ping start\")\n    async with send:\n        sleep_time = uniform(0, 3)\n        await anyio.sleep(sleep_time)\n        await send.send((f\"ping from {num=}, {sleep_time=}\", sleep_time))\n    print(f\"{datetime.today()}:: {num=}, ping end\")\n\n\nasync def pong(receive: ObjectReceiveStream):\n    sleep_time_sum = 0\n    print(f\"{datetime.today()}:: pong start\")\n    async for text, sleep_time in receive:\n        print(f\"{datetime.today()}:: pong:: {text}\")\n        sleep_time_sum += sleep_time\n    print(f\"{datetime.today()}:: pong end:: {sleep_time_sum=}\")\n\n\nasync def main():\n    async with anyio.create_task_group() as task_group:\n        send, receive = anyio.create_memory_object_stream(0)\n        async with send:\n            for num in range(5):\n                task_group.start_soon(ping, num, send.clone())\n        async with receive:\n            task_group.start_soon(pong, receive.clone())\n\n\nif __name__ == \"__main__\":\n    result = anyio.run(main)\n```\n\n우선 `main`함수부터 확인하겠습니다.\n\n`create_task_group`으로 `task_group` 블록을 생성합니다. 그리고 `create_memory_object_stream`으로 각 작업간 정보를 주고받기 위한 `stream`을 생성합니다.\n\n\u003e 이때 생성된 `stream`은 send와 receive 두개로 나뉘어 반환됩니다.\n\n이어서 5개의 `ping`과 1개의 `pong` 작업을 호출해서, 실제로 `main`을 실행할 때 실행될 작업을 지정합니다.\n\n여기서 사용되는 `async with send`와 `async with receive`는 안전한 프로그래밍을 위해 필요한 문법으로, 해당 블록이 끝나면 따로 `close` 메소드를 호출 할 필요 없이, 자동으로 해당 객체를 닫습니다.\n\n그리고 `start_soon`으로 호출되는 각 작업에 send와 receive를 보낼 때 사용되는 `clone`메소드는, 해당 `stream`의 새로운 send와 receive를 생성해서 보내기 위해서 사용됩니다. 만약 send나 receive를 사용하는 작업이 하나가 아닐 때 `clone` 메소드를 사용하지 않는다면, 의도치 않은 `ClosedResourceError`를 만나게 될 수 있습니다.\n\n`main`에서 호출된 작업은 간단합니다.\n`ping`은 0~3초 사이의 랜덤한 시간동안 대기한 다음, 해당 시간에 대한 정보를 send를 이용해서 보냅니다.\n`pong`은 receive를 통해 받은 정보를 출력하고, 합산합니다.\n또한, 각 `ping` 작업은 `main`에서 사용된 방식과 같은 방식으로 `async with` 블록으로 구성되어 있으므로, 따로 `close` 메소드를 호출 할 필요가 없습니다.\n\n이 방식은 상당히 유용한데, 모든 send가 닫히고, 더이상 버퍼에 정보가 없다면 `async for` 블록 또한 자동으로 닫히게 됩니다.\n\n위 코드를 실행해보면 다음과 같은 출력을 확인할 수 있습니다.\n\n```log\n2021-11-29 18:34:55.095995:: num=0, ping start\n2021-11-29 18:34:55.096041:: num=1, ping start\n2021-11-29 18:34:55.096056:: num=2, ping start\n2021-11-29 18:34:55.096068:: num=3, ping start\n2021-11-29 18:34:55.096080:: num=4, ping start\n2021-11-29 18:34:55.096091:: pong start\n2021-11-29 18:34:55.348744:: num=4, ping end\n2021-11-29 18:34:55.348931:: pong:: ping from num=4, sleep_time=0.25142686937439906\n2021-11-29 18:34:56.042114:: num=3, ping end\n2021-11-29 18:34:56.042310:: pong:: ping from num=3, sleep_time=0.9448073290170278\n2021-11-29 18:34:56.779485:: num=1, ping end\n2021-11-29 18:34:56.779686:: pong:: ping from num=1, sleep_time=1.6822150526554853\n2021-11-29 18:34:56.925375:: num=2, ping end\n2021-11-29 18:34:56.925595:: pong:: ping from num=2, sleep_time=1.8275787340996095\n2021-11-29 18:34:57.535689:: num=0, ping end\n2021-11-29 18:34:57.535911:: pong:: ping from num=0, sleep_time=2.4386441219878816\n2021-11-29 18:34:57.536030:: pong end:: sleep_time_sum=7.144672107134403\n```\n\n바이트 스트림 또한 대동소이합니다.\n\n요약하자면\n\n\u003e 1. 바이트 스트림과 객체 스트림으로 정보를 주고받을 수 있다.\n\u003e 2. send와 receive 두개로 나누어 사용한다.\n\u003e 3. send 또는 receive를 사용하는 작업이 여러개라면 `clone` 메소드를 사용한다.\n\u003e 4. `async with`, `async for`를 사용하면 좀 더 깔끔하고 명확하게 작성할 수 있다.\n","mtime":"2022-08-13T00:12:20.000+09:00","href":"velog/파이썬으로 동시성 프로그래밍을 쉽게 하는법 - 2","data":{"title":"파이썬으로 동시성 프로그래밍을 쉽게 하는법 - 2","date":"2021-11-29T18:42:54.419+09:00","tags":["anyio","async","python","@all"],"page":"파이썬 동시성 프로그래밍","summary":"anyio로 작업 생성 + 작업간 정보 주고받기"}}]},"__N_SSG":true},"page":"/posts/@tag/[...tag]","query":{"tag":["@all","4"]},"buildId":"MQ_lMVQgBI0RkmUA6wvVT","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>